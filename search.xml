<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[rabbitmq-routingkey路由]]></title>
    <url>%2F2018%2F05%2F03%2Frabbitmq-directrouting%2F</url>
    <content type="text"><![CDATA[前言上一节我们介绍了发布订阅模式使用Binding把Exchange转发器与queue队列联系起来了，使得Exchange知道要把消息发送到那些queue中，但是Exchange会把所有的消息都发送到与之关联的queue队列中，如果我们的queue只想接收跟它绑定的Exchange的某一类消息，能否实现呢？ Direct Bindings routing key上一节中我们我们介绍的Binding是这样写的1channel.queue_bind(exchange='logs', queue=queuename) 其实后面还可以增加一个参数 routing_key1channel.queue_bind(exchange='logs', queue=queuename, routing_key='orange') 这样与名字叫logs的Exchange转发器绑定的该queue队列，现在只会接收到orange类型的消息了，Exchange转发器发送消息的时候会根据发送消息时填的routing_key的值找到完全比配的Binding是填写的routing_key的值的队列，然后把消息发送到这些队列中去。routing_key参数只对direct与topic类型的Exchange适用，fanout类型的Exchange使用routing_key不起作用。如何我们可以看出类型为direct的Exchange X与Q1,Q2队列绑定了，Q1的routing_key是 orange，Q2的routing_key是black,green两类消息，当Producer推送消息的routing_key是orange的时候，Exchagne X会把消息推送给Q1,如果Producer推送消息的routing_key是black或者green时会把消息推送给Q2，如果不是orange，black，green那么消息就会丢弃不会推送给这两个queue。同一个队列如何指定多个不同的routing_key呢？12channel.queue_bind(exchange='logs', queue=queuename, routing_key='green')channel.queue_bind(exchange='logs', queue=queuename, routing_key='black') queue_bind的时候指定不同的routing_key就可以了。 Multiple Bindings routing key多个queue使用相同的routing_key继续绑定也时可以的,如图类型为direct的Exchagne X与Q1,Q2队列绑定了，这两个队列的routing_key都时black,所以当Producer推送的消息的routing_key是black的时候，会同时推送给Q1,Q2队列，其他routing_key不是black的消息会丢弃掉。 Publish Message发送消息的时候跟之前的代码类似，只是我们需要多加要给routing_key参数1234#定义direct类型的exchagnechannel.exchange_declare(exchange='logs', exchange_type='direct')#发送消息的时候指定routing_keychannel.basic_publish(exchange='logs', routing_key='orange', body=message) 完整实例Producer：12345678910111213141516171819202122232425262728#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='direct_logs', exchange_type='direct')for num in range(1, 7): time.sleep(2) if num &lt;= 2: severity = 'orange' elif num &lt;= 4: severity = 'black' else: severity = 'green' message = 'send logs routing_key=%s' % (severity,) channel.basic_publish(exchange='direct_logs', routing_key=severity, body=message, properties=pika.BasicProperties(delivery_mode=2)) print(" [x] Sent %r" % (message,))connection.close() Consumer1 routing_key为orange12345678910111213141516171819202122232425262728293031#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='direct_logs', exchange_type='direct')result = channel.queue_declare(exclusive=True)queuename = result.method.queuechannel.queue_bind(exchange='direct_logs', queue=queuename, routing_key='orange')print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(2) print('[x] done') ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue=queuename)channel.start_consuming() Consumer2 routing_keyi为green和black1234567891011121314151617181920212223242526272829303132#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='direct_logs', exchange_type='direct')result = channel.queue_declare(exclusive=True)queuename = result.method.queuechannel.queue_bind(exchange='direct_logs', queue=queuename, routing_key='green')channel.queue_bind(exchange='direct_logs', queue=queuename, routing_key='black')print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(2) print('[x] done') ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue=queuename)channel.start_consuming() 打印的日志： 通过管理界面查看Exchange与队列的Bindings关系]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-发布/订阅]]></title>
    <url>%2F2018%2F05%2F01%2Frabbitmq-publishSubscribe%2F</url>
    <content type="text"><![CDATA[前言不知道大家注意到没，前面的文章我们发送消息的时候都是这么写的12channel.queue_declare(queue='cache_hello')channel.basic_publish(exchange='', routing_key='cache_hello', body=message) 关注第一个和第二个参数，第一个参数exchange我们赋值的是空字符串，第二个参数outing_key我们赋值的’cache_hello’，这个是我们定义的queue的名字。当我们给exchange赋值空字符串的时候，表示使用RabbitMQ Server的默认Exchange转发器，消息会根据我们指定的routing_key推送到与该值对应名字的消息队列中。我们可以看看管理页面里RabbitMQ Server定义的默认Exchange的描述默认的消息队列是支持持久化的并且是Direct类型的转发器。Bindings栏的英文表示默认Exchange转发器可以隐式绑定到与routing_key的值相同名字的队列上，默认Exchange转发器不能进行显示绑定或者解绑操作，也不能进行删除操作。我们之前的实例代码都是把消息推送到一个指定的queue队列中，接下来我们尝试使用“发布/订阅”把消息推送给多个队列，我们构建一个简单的日志系统来验证这种模式，Producer负责收集系统日志然后发出日志，Consumer负责接受日志并且打印日志，我们将构建两个Consumer，一个负责输出日志到屏幕，一个负责把日志写到磁盘上。 Exchange转发器首先我们来看看之前文章的AMQP组件流向图从图中可以看出Producer并不是直接把消息发送到queue队列中的，Producer并不知道它发送的消息要发送到哪个queue队列以及是否已经到达queue队列。消息是通过Exchange转发器进行转发到queue队列中的，Exchange与queue队列又是通过Bindings进行绑定配置的。所以Exchange需要知道如何处理消息，该把消息放置到一个queue队列还是多个queue队列，这个规则是基于Exchange的类型决定的。我们前面的文章里介绍了Exchange转发器有三种类型：direct，topic，fanout。1234#定义fanout类型的Exchange转发器channel.exchange_declare(exchange='logs', exchange_type='fanout')#发送消息channel.basic_publish(exchange='logs', routing_key='', body=message) 可以通过上面代码中的exchange_declare方法定义Exchagne，exchange=’logs’参数表示我们定义的Exchange的名字，exchange_type=’fanout’表示我们定义的fanout类型(广播模式)的转发器。basic_publish方法发送消息，我们指定了exchange为我们定义的logs转发器，并没有指定routing_key参数，因为我们要广播发送给与logs转发器绑定的所有queue队列。 Temporary queues（临时队列）之前我们使用的队列都是提前定义的有具体名字的队列，当我们使用有具体名字的队列的时候，表示我们的所有的Consumer消费者都关心该队列里的消息，并且每个消费者只能获取里面的部分消息，因为同一个消息只会发送给与该队列连接的一个消费者。但是在这里的日志系统里，我们每个新连接的Consumer消费者需要接受所有的日志消息，并且只关心当前的日志消息，在该Consumer连接前的日志却并不关系。所以我们需要在每个新的Consumer进行连接的时候申明一个新queue队列，名字我们并不关心，因为只有该consumer使用，并且该queue在该Consumer关闭的时候可以自动的删除。1234#定义新队列 exclusive=True 表示关闭连接的时候自动删除该queue对立result = channel.queue_declare(exclusive=True)#获取临时队列的名称queuename = result.method.queue 使用上面的代码可以为我们创建一个会自动删除的临时队列，队列的名字由RabbitMQ Server随机命名，比如：’amq.gen-Zg3NrcqQg5gSupdVB0AtCA’ Bindings（绑定）我们创建了fanout类型的Exchange还有临时队列，现在需要把两者关联起来，这个就需要用到Bindings了，不然Exchange怎么知道把消息发送给哪个queue呢。1channel.queue_bind(exchange='logs', queue=queuename) 完整实例Producer：1234567891011121314151617181920#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='logs', exchange_type='fanout')for num in range(1, 7): time.sleep(2) message = '[%s]-log-error-%s' % (time.strftime('%H:%M:%S', time.localtime(time.time())), str(num)) channel.basic_publish(exchange='logs', routing_key='', body=message) print(" [x] Sent %r" % (message,))connection.close() Consumer 模拟打印日志到屏幕：12345678910111213141516171819202122232425262728293031#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='logs', exchange_type='fanout')result = channel.queue_declare(exclusive=True)queuename = result.method.queue#绑定Exchange与queuechannel.queue_bind(exchange='logs', queue=queuename)print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] 模拟打印日志到屏幕 %r' % (body,)) print('[x] done') ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue=queuename)channel.start_consuming() Consumer 模拟写入日志到磁盘：12345678910111213141516171819202122232425262728293031#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='logs', exchange_type='fanout')result = channel.queue_declare(exclusive=True)queuename = result.method.queue#绑定Exchange与queuechannel.queue_bind(exchange='logs', queue=queuename)print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] 模拟写入日志到磁盘 %r' % (body,)) print('[x] done') ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue=queuename)channel.start_consuming() 两个Consumer代码基本一样就是模拟打印print的文字不同而已，我们需要首先启动Consumer，如果先启动Producer是没有效果的，我们先启动一个Consumer，然后启动Producer程序，Producer会发送6条日志消息，由于我们设置了每发送一条消息休眠2秒钟，所以当发送了3条的时候，我们马上开启另外一个Consumer，可以从打印的日志看到，我们后开启的Consumer会从新日志开始全部接收。打印的日志： 看看我们管理界面里出现了两个随机命名的queue队列：]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-消息确认机制与持久化存储]]></title>
    <url>%2F2018%2F04%2F29%2Frabbitmq-ackdurable%2F</url>
    <content type="text"><![CDATA[ACK 消息确认机制上一节我们介绍了RabbitMQ的分发机制有Round-robin（循环分发）与Fair dispatch（公平分发），其中的公平分发机制就必须使用到消息确认机制来保证RabbitMQ Server知道Consumer已经处理完成了任务，从而继续发送新任务给该Consumer处理，RabbitMQ Server没有使用超时机制，仅通过Consumer的连接是否中断，如果中断前还没有收到确认消息表示消息没有被成功处理。 使用消息确认机制还能保证消息不丢失，Consumer成功处理消息后发送确认消息，RabbitMQ Server才会任务消息被成功处理，然后才会把消息标记为已完成从queue中删除，如果Consumer异常或者未发送确认消息，RabbitMQ则认为消息并没有成功被处理，后续会再发送给其他Consumer进行处理，直到收到对应的确认消息，这样就能保证消息不丢失，能被正真的处理。 如果不使用确认机制，那么RabbitMQ Server 发送的消息Consumer接收以后，不管Consumer是否处理完成，直接把消息标记为完成从queue队列中删除。 我们继续使用上面的公平分发里的Consumer代码来模拟一个处理过程中关闭程序，看下消息是否还保存再队列中，该代码里我们已经开启了手动消息确认机制(默认打开) ，然后回调函数的ch.basic_ack(delivery_tag=method.delivery_tag)这句代码表示确认消息。我们使用上一节开头提供的Producer代码发送一条消息，然后开启一个Consumer可以把休眠时间设置长一点10秒把，再任务未处理完成前关闭程序，然后到管理界面查看消息是否还存在。 开启Producer发送一条消息： 管理界面 hello队列中可以看到刚才1条刚才发送的消息： 开启Consumer接受消息进行处理 可以看到消息并没有处理完成因为没有打印 [x] done ： 当RabbitMQ Server把消息发送给Consumer的后再没有接受到应答前 管理界面 hello队列里的该消息被标记为Unacked 当我再Consumer未处理完成返回确认消息前关闭程序,那么管理界面 hello队列里的该消息又会重新被标记为Ready 等其他Consumer连接到hello队列的时候，会再次发送该消息。 持久化存储消息确认机制虽然可以从消息处理流程上保证消息不丢失，但这只仅限于RabbitMQ Server正确运行的条件下，万一我们的RabbitMQ Server异常了或者我们的服务器重启了，由于我们的队列以及消息数据默认保存在内存中，所以肯定就丢失了。所以我们必须开启消息持久化能力，让数据能保存到磁盘上，防止数据的丢失。首先我们在创建队列的时候要指定为持久化，加入参数durable=True1channel.queue_declare(queue='hello', durable=True) 如果我们已经存在名称为hello的队列，那么这个语句不会起作用，可以到管理页面找到该队列，进入该队列的明细页面先删除该队列。然后发送消息的时候也要指明持久化123456channel.basic_publish(exchange='', routing_key="hello", body=message, properties=pika.BasicProperties( delivery_mode = 2, #设置消息持久化 )) 虽然我们使用消息确认机制跟持久化能保证消息不丢失，但是这个并不是百分之百的能保证所有消息不丢失，RabbitMQ Server 并不是收到收到每一条消息就同步进行持久化操作写入磁盘的，这样肯定会验证的影响性能，可能是先缓存起来了还没来得及写入磁盘，那么这个时候crash掉了，这部分还没有来得及存储的消息肯定就会丢失掉。 代码示例Producer：1234567891011121314151617181920#!/usr/bin/env pythonimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.0.110', port='5672', credentials=user_pwd))channel = connection.channel()#定义队列设置持久化durable=Truechannel.queue_declare(queue='hello', durable=True)for num in range(1, 2): message = 'hello world ' + str(num) #发送消息设置持久化delivery_mode=2 channel.basic_publish(exchange='', routing_key='hello', body=message, properties=pika.BasicProperties(delivery_mode=2)) print(" [x] Sent %r" % (message,))connection.close() Consumer：12345678910111213141516171819202122232425262728#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.0.110', port='5672', credentials=user_pwd))channel = connection.channel()channel.queue_declare(queue='hello', durable=True)channel.basic_qos(prefetch_count=1)print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(1) print('[x] done %s' % (time.strftime('%H:%M:%S', time.localtime(time.time())),)) ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue='hello')channel.start_consuming() 调用一次Producer程序然后查看下管理界面的hello队列，看到创建的hello队列有个Features为durable:true表示为持久化队列，里面存储了一条我们刚刚发送的消息 我们可以重启RabbitMQ Server看看我们的hello消息队列以及里面的那条消息是否还在。我们要持久化发送的消息的前提必须首先保证我们的消息队列是可持久化的。]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-分发机制]]></title>
    <url>%2F2018%2F04%2F28%2Frabbitmq-distribution%2F</url>
    <content type="text"><![CDATA[前言上一节的最后部分展示了一个基本的RabbitMQ发送接收的Hello World程序，Porducer发送消息，Consumer订阅队列接受消息然后打印消息，但是在实际应用场景中可能更加复杂些，如此简单的发送接受并不能满足实际需求，有可能Consumer端会有耗时计算等情况存在，所以RabbitMQ Server需要有一定的发送机制来平衡每个Consumer的负载，还有消息如何持久化，如何保证消息投递成功不丢失等 分发机制我们先模拟下Consumer进行耗时操作，看看消息是如何分发的Producer：123456789101112131415161718#!/usr/bin/env pythonimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='172.16.200.122', port='5672', credentials=user_pwd))channel = connection.channel()channel.queue_declare(queue='hello')for num in range(1, 8): message = 'hello world ' + str(num) channel.basic_publish(exchange='', routing_key='hello', body=message) print(" [x] Sent %r" % (message,))connection.close() Consumer：12345678910111213141516171819202122232425#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='172.16.200.122', port='5672', credentials=user_pwd))channel = connection.channel()channel.queue_declare(queue='hello')print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(5) print('[x] done %s' % (time.strftime('%H:%M:%S',time.localtime(time.time())),)) channel.basic_consume(callback, queue='hello', no_ack=True)channel.start_consuming() 上面的代码Consumer代码使用time.sleep(5)使线程休眠5秒来模拟耗时操作，Producer使用range(1, 8) 循环发送了7个消息，我开启了两个Consumer来接受消息，我们来看看Consumer打印的日志Consumer1Consumer2Producer Round-robin（循环分发）从打印的日志可以看出两个Consumer轮流消费了发送的7个消息，在默认的设置下，RabbitMQ会逐个发送消息到注册了该队列的消费者列表的下一个消费者，已经提前一次性分配，并不会考虑消费者处理能力的快慢，我们可以尝试把其中一个Consumer的休眠时间设置长一点，会发现两个Consumer还是在轮流的接受消息，这种处理方式叫轮询分发RabbitMQ默认使用的就是轮询分发机制，当任务加重Consumer处理不过来的时候，可以通过创建更多的Consumer来分摊任务的处理 Fair dispatch（公平分发）循环分发虽然可以通过增加更多的Consumer来增加处理消息的能力，但是循环分发并不能监控的Consumer的处理能力，它只管逐个的发送消息，这样有可能有的Consumer处理能力强，有的Consumer处理能力弱点，结果处理能力强的Consumer把消息处理完后空闲了，处理能力弱的Consumer还在艰难的处理消息。下面我把上面的例子稍微修改下，把其中一个Consumer的休眠时间修改为5秒time.sleep(5)，其他代码不变看下答应的日志情况Consumer休眠1秒：Consumer休眠5秒：可以看到休眠1秒的在28秒的时候就已经处理完了分配的4个任务，休眠5秒的第一个任务处理完已经是29秒了，最后一个39秒处理完成。可见分配并不太合理。所有RabbitMQ为我们提供了另一种分配机制公平分发通过在Consumer端设置 channel.basic_qos(prefetch_count=1), 并且在处理完任务后设置ch.basic_ack(delivery_tag=method.delivery_tag)，RabbitMQ就会在接受到该Consumer的ack应答钱不会在把新的任务给该Consumer了 Consumer代码：12345678910111213141516171819202122232425262728#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='172.16.200.122', port='5672', credentials=user_pwd))channel = connection.channel()channel.queue_declare(queue='hello')channel.basic_qos(prefetch_count=1)print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(5) print('[x] done %s' % (time.strftime('%H:%M:%S', time.localtime(time.time())),)) ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue='hello')channel.start_consuming() channel.basic_consume(callback, queue=’hello’) 默认是开启手动ack应答的，加入参数no_ack=True表示RabbitMQ Server 发送的消息Consumer接收以后，不管Consumer是否处理完成，直接把消息标记为完成从queue队列中删除。不过这种分发机制有个问题就是如果所有的Consumer都繁忙了，当需要处理的消息持续累积，队列有可能被填满，这种情况需要关注队列使用情况，合理安排Consumer的数量，或者使用别的策略如果代码中任务处理完成后忘记调用ack应答，那么会导致RabbitMQ Server不再把消息发送给该Consumer处理，那么也会导致消息队列被塞满，这个错误需要避免。 我们来看看设置公平策略后打印的日志情况，Producer还是沿用之前的代码，Consumer修改为上面提供的代码，注意修改两个Consumer休眠时间使其不一样，模拟处理能力的不同Consumer休眠1秒：Consumer休眠5秒：可以看到两个Consumer打印的日志休眠1秒处理的任务明显增多，并且从打印的时间上来看也是满足任务处理能力的。]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq 简介]]></title>
    <url>%2F2018%2F04%2F27%2Frabbitmq-introduce%2F</url>
    <content type="text"><![CDATA[AMQP背景AMQP，即Advanced Message Queuing Protocol,一个提供统一消息服务的应用层标准高级消息队列协议,是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同的开发语言等条件的限制，与2004年由摩根大通跟iMatrix开始着手该协议的开发，2006年AMQP规范发布，2007年，Rabbit公司基于AMQP标准开发的RabbtiMQ1.0发布，使用Erlang语言开发，Erlang语言由Ericson设计，专门为开发concurrent和distribution系统的一种语言。 AMQP基本概念 Broker：接受和分发消息的应用，RabbitMQ Server 就是 Message Broker Virtual Host：出于多租户和安全因素的设计，把AMQP的基本组件划分到了一个个的虚拟分组中，当多个不同的用户使用同一个RabbitMQ Server提供的服务的时候，可以划分多个Virtual host，每个用户在自己的Virtual host创建Exchange/queue等等，起到隔离作用。 Connection：Publisher/Consumer和Broker之间通过TCP连接，断开连接的操作只会在Client端，Broker不会断开，除非出现网络故障或者Broke服务异常 Channel：如果每次访问RabbitMQ都建立一个Connection连接，消息量大的时候建立TCP Connection连接开销是非常大的，效率也比较低，Channel是在Connection内部建立的逻辑连接，如果应用程序支持多线程，每个线程通常会创建单独的Channel今昔通讯，AMQP method包含了 Channel Id 帮助客户端和Message Broker识别Channel，所以不同的Channel之间是安全隔离的，Channel作为轻量级的Connection极大的减少了操作系统建立TCP Connection的开销 Exchange：message到达Broker的第一站，根据配置的分发规则，匹配查询表里的routing key，分发消息到匹配的的queue中，常用的类型有direct (point-to-point)，topic (publish-subscribe) and fanout (multicast)，topic类型的routing key支持*(匹配一个单词) #(匹配0个或者多个单词)两个通配符 Queue：消息最终配存放的地方，等待被consumer取走，一个message可以被同时拷贝到多个queue中 Binding：Exchange和queue之间的虚拟连接，binding中可以包含routing key。Binding信息被保存到exchange中的查询表中，用于message的分发依据 组件流向图 Producer Consumer Producer推送消息到Broker中，Broker内部创建好了Exchange Queue，并通过Binding将两者关联起来，Echange分发消息的时候会根据自身的类型以及Binding不同的分发策将消息放入匹配的Queue队列中，等待Consumer取走。 Exchange三种类型 Direct Direct类型的Exchange会根据发送消息时传递的routing_key去查找Binding时设置有相同的routing_key的关联队列，然后把消息存到关联的队列中，然后等待Consumer到队列中消费消息,如果没有找到关联队列，消息会被丢弃 Topic Topic类型的Exchange会根据routing key 以及通配规则，按照规则找到匹配的绑定的queue中，routing key 的值通常设置多个单词使用.连接routing key中包含两种通配符：# 可以匹配零个或者多个单词* 匹配任意单个单词 Fanout Fanout类型的Exchagne会把消息广播到所有绑定的queue中去 简单示例 Hello WorldProducer1234567891011121314151617#!/usr/bin/env pythonimport pika#连接RabbitMQ的账号密码user_pwd = pika.PlainCredentials(username='admin', password='123123')#创建connection连接connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.11.176', port='5672', credentials=user_pwd))#创建channelchannel = connection.channel()#定义queue 取名 hello，使用前最好都重新定义，如果队列已经存在会忽略该动作channel.queue_declare(queue='hello')#发送消息 使用默认的exchange 设置 routing key 为 hello，hello就是上面创建队列的名字channel.basic_publish(exchange='', routing_key='hello', body='hello world')print('[x] send hello world')connection.close() Consumer12345678910111213141516171819202122#!/usr/bin/env pythonimport pika#连接RabbitMQ的账号密码user_pwd = pika.PlainCredentials(username='admin', password='123123')#创建connection连接connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.11.176', port='5672', credentials=user_pwd))#创建channelchannel = connection.channel()#定义queue 取名 hello，使用前最好都重新定义，如果队列已经存在会忽略该动作channel.queue_declare(queue='hello')print(' [*] Waiting for messages. To exit press CTRL+C')#定义接受消息的回调函数def callback(ch, method, properties, body): print('[x] receive %r' % (body,))#设置监听名字为hello队列channel.basic_consume(callback, queue='hello', no_ack=True)#开启监听channel.start_consuming() 参考资料 http://www.cnblogs.com/frankyou/p/5283539.html https://blog.csdn.net/anzhsoft/article/details/19563091]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ 安装]]></title>
    <url>%2F2018%2F04%2F26%2Frabbitmq-install%2F</url>
    <content type="text"><![CDATA[安装 rabbitmq-server1sudo apt-get install rabbitmq-server 添加用户 添加用户设置用户名密码 1sudo rabbitmqctl add_user username password 将用户设置为管理员（只有管理员才能远程登录） 1sudo rabbitmqctl set_user_tags username administrator 设置用户的读写等权限 1sudo rabbitmqctl set_permissions -p / username ".*" ".*" ".*" 安装RabbitMQ监控管理插件1sudo rabbitmq-plugins enable rabbitmq_management 启动插件后可以通过地址 http://localhost:15672 登陆RabbitMQ管理页面，对RabbitMQ进行管理。 基本命令启动1sudo service rabbitmq-server start 或1sudo rabbitmq-server –detached 关闭1sudo service rabbitmq-server stop 1sudo rabbitmqctl stop 插件管理开启某个插件：1rabbitmq-plugins enable xxx 关闭某个插件：1rabbitmq-plugins disable xxx 用户管理新建用户：1rabbitmqctl add_user xxx pwd 删除用户:1rabbitmqctl delete_user xxx 改密码:1rabbimqctl change_password &#123;username&#125; &#123;newpassword&#125; 设置用户角色：1rabbitmqctl set_user_tags &#123;username&#125; &#123;tag ...&#125; Tag可以为 administrator,monitoring, management 权限设置：1rabbitmqctl set_permissions [-pvhostpath] &#123;user&#125; &#123;conf&#125; &#123;write&#125; &#123;read&#125; 服务器状态：1rabbitmqctl status 队列信息：1rabbitmqctl list_queues[-p vhostpath] [queueinfoitem ...] Queueinfoitem可以为：name，durable，auto_delete，arguments，messages_ready，messages_unacknowledged，messages，consumers，memory Exchange信息：1rabbitmqctllist_exchanges[-p vhostpath] [exchangeinfoitem ...] Exchangeinfoitem有：name，type，durable，auto_delete，internal，arguments. Binding信息：1rabbitmqctllist_bindings[-p vhostpath] [bindinginfoitem ...] Bindinginfoitem有：source_name，source_kind，destination_name，destination_kind，routing_key，arguments Connection信息：1rabbitmqctllist_connections [connectioninfoitem ...] Connectioninfoitem有：recv_oct，recv_cnt，send_oct，send_cnt，send_pend等。 Channel信息：1rabbitmqctl list_channels[channelinfoitem ...] Channelinfoitem有consumer_count，messages_unacknowledged，messages_uncommitted，acks_uncommitted，messages_unconfirmed，prefetch_count，client_flow_blocked RabbitMQ管理界面]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 单机多实例部署]]></title>
    <url>%2F2018%2F01%2F08%2Fdocker-elk-two%2F</url>
    <content type="text"><![CDATA[前言 上一篇文章docker下的elk环境搭建我们介绍了docker下的elk各个组件的部署，走通了从采集日志到查询日志的流程，但是我们之前部署的elasticsearch组件采用的单实例部署，这样并不能很好的利用系统资源，并且也不安全，如果仅有的一个实例出现了问题那么整个服务就瘫痪了，而且数据配置等都保存在docker容器中，这样也不利于我们管理维护，所以接下来我们就要解决这两个问题。 解决方案 elasticsearch 设置多个配置文件，一个实例对应一个配置文件 使用docker的数据卷功能将服务器的路径映射到docker中，使elasticsearch的数据和配置直接保存到服务器中 实施步骤docker 容器启动 我们使用数据卷功能将服务器的路径映射到我们要启动的docker容器内1$ docker run -p 5044:5044 -p 8080:80 -p 9200:9200 -p 9100:9100 -v /home/tiger/Downloads/elk:/home/tiger/elk -d elk:ubuntu /run.sh -v 参数就是启用数据卷配置，/home/tiger/Downloads/elk:/home/tiger/elk 表示把服务器的/home/tiger/Downloads/elk目录映射到docker容器的/home/tiger/elk目录上，那么你在容器内访问/home/tiger/elk里的内容实际上就是在访问服务器的/home/tiger/Downloads/elk内的内容。 elasticsearc 配置数据以及配置文件路径创建 我们在服务器上建立专门存放elasticsearch数据跟配置的文件夹,/home/tiger/Downloads/elk/elasticsearch是我自己建的路径，这个你们可以按照各自情况创建路径123$ mkdir -p /home/tiger/Downloads/elk/elasticsearch$ cd /home/tiger/Downloads/elk/elasticsearch$ mkdir -p ./elasticsearch/node1 ./elasticsearch/node2 我们只做双实例的配置所以我建立了两个节点的文件夹 node1、node2，多个实例以此类推，配置都是类似的。 然后我们进入docker容器，把elasticsearh的默认配置拷贝到这两个节点文件夹里12345$ docker start 35d535f88f45 --启动容器$ docker exec -it 35d /bin/bash -- 附加到容器root@35d535f88f45:/# su tiger --切换用户tiger@35d535f88f45:/$ tiger@35d535f88f45:/$ cd /opt/elasticsearch-6.1.1/ --进入elasticsearch目录 我们现在开始拷贝配置，把elasticsearch里的config、logs、data文件夹都拷贝到node1、node2文件夹中12tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ cp -r ./config ./data ./logs ~/elk/elasticsearch/node1/ --拷贝到node1tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ cp -r ./config ./data ./logs ~/elk/elasticsearch/node2/ --拷贝到node2 现在我们看看我们服务器刚才创建的/home/tiger/Downloads/elk/elasticsearch目录下的node1 和node2里面的内容123456789101112 $ ll node1 node2node1:总用量 12drwxr-xr-x 2 tiger tiger 4096 1月 8 00:15 configdrwxrwxr-x 3 tiger tiger 4096 1月 8 09:37 datadrwxrwxr-x 2 tiger tiger 4096 1月 8 09:37 logsnode2:总用量 12drwxr-xr-x 2 tiger tiger 4096 1月 8 09:46 configdrwxrwxr-x 3 tiger tiger 4096 1月 8 09:50 datadrwxrwxr-x 2 tiger tiger 4096 1月 8 09:50 logs config、data、logs已经复制进去了 各节点elasticsearch.yml配置 现在可以配置node1 跟node2 文件夹里config下的elasticsearch.yml文件了，config里拷贝过来了三个文件12345 $ ll ./node1/config 总用量 16-rw-rw---- 1 tiger tiger 3841 1月 8 00:15 elasticsearch.yml-rw-rw---- 1 tiger tiger 2672 1月 7 22:58 jvm.options-rw-rw---- 1 tiger tiger 5091 1月 7 22:58 log4j2.properties node1 节点的elasticsearch.yml文件配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:# es默认的集群名称是elasticsearch，注意，集群中是以name来区分节点属于那个集群的。cluster.name: tiger-elasticsearch## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#node.name: node-1## Add custom attributes to the node:##node.attr.rack: r1#是否让这个节点作为默认的master，若不是，默认会选择集群里面的第一个作为master，es有一套选择那个节点作为master的机制node.master: true# ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#配置节点数据存放的目录path.data: /home/tiger/elk/elasticsearch/node1/data## Path to log files:#配置节点日志存放的目录path.logs: /home/tiger/elk/elasticsearch/node1/logs## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 0.0.0.0## Set a custom port for HTTP:#http.port: 9200## For more information, consult the network module documentation.##es集群节点之间的通信端口号。默认9300.transport.tcp.port: 9300# --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]#集群多播时发现其他节点的主机列表， 真实多机集群环境下，这里会是多个主机的IP列表，默认格式“host：port”的数组discovery.zen.ping.unicast.hosts: ["127.0.0.1:9300"]## Prevent the "split brain" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: ## For more information, consult the zen discovery module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true#默认情况下，是不建议单机启动多个node的，这里这个参数，就是告知es单机上启动了几个实例，这里我们配置2个，若是要配置3个或者更多实例，修改这个数字即可node.max_local_storage_nodes: 2# plugs config http.cors.enabled: true http.cors.allow-origin: "*" node2 节点的elasticsearch.yml文件配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:#cluster.name: tiger-elasticsearch## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#node.name: node-2## Add custom attributes to the node:##node.attr.rack: r1#node.master: false# ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /home/tiger/elk/elasticsearch/node2/data## Path to log files:#path.logs: /home/tiger/elk/elasticsearch/node2/logs## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 0.0.0.0## Set a custom port for HTTP:#http.port: 9201## For more information, consult the network module documentation.#transport.tcp.port: 9301# --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]#discovery.zen.ping.unicast.hosts: ["0.0.0.0:9300"]## Prevent the "split brain" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: ## For more information, consult the zen discovery module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: truenode.max_local_storage_nodes: 2# plugs config http.cors.enabled: true http.cors.allow-origin: "*" elasticsearch服务启动 我网上查资料说启动elasticsearch服务的时候指定配置文件路径使用的参数是 -Des.path.conf ，我发现不行，后来网上看说是用 -Epath.conf 我测试也不行，查了好久资料都没有解决，后来看官网说可以设置ES_PATH_CONF环境变量来设置配置参数 启动 node1，打印正常日志就表示启动好了123tiger@35d535f88f45: cd /opt/elasticsearch-6.1.1tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ export ES_PATH_CONF=/home/tiger/elk/elasticsearch/node1/config --设置ES_PATH_CONF到你的配置路径文件夹tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ ./bin/elasticsearch -p /home/tiger/elk/elasticsearch/node1.pid --启动 启动 node2，打印正常日志就表示启动好了123tiger@35d535f88f45: cd /opt/elasticsearch-6.1.1tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ export ES_PATH_CONF=/home/tiger/elk/elasticsearch/node2/config --设置ES_PATH_CONF到你的配置路径文件夹tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ ./bin/elasticsearch -p /home/tiger/elk/elasticsearch/node2.pid --启动 如果发现其他便捷的启动方式可以告诉下我。 结束 我们请求 http://localhost:9200/_cluster/health 可以看到返回了1234567891011121314151617&#123;cluster_name: "tiger-elasticsearch",status: "green",timed_out: false,number_of_nodes: 2,number_of_data_nodes: 2,active_primary_shards: 9,active_shards: 18,relocating_shards: 0,initializing_shards: 0,unassigned_shards: 0,delayed_unassigned_shards: 0,number_of_pending_tasks: 0,number_of_in_flight_fetch: 0,task_max_waiting_in_queue_millis: 0,active_shards_percent_as_number: 100&#125; status 状态是 green 表示健康，因为我们有了双实例，有一个充当了副本做了备份。number_of_nodes: 2 表示有两个节点 我们使用的是单机做实验，正式使用肯定是多台服务器做分布式部署，才能真正发挥威力。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>elasticsearch</tag>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker下的elk环境搭建]]></title>
    <url>%2F2018%2F01%2F04%2Fdocker-elk-create%2F</url>
    <content type="text"><![CDATA[elk结构图 先来张elk的结构图，对我们将搭建的环境有个基本的了解 用户通过Nginx反向代理请求到Kibana进行Web操作，Kibana封装了Elasticsearch，提供Web接口进行可视化操作，Logstash进行日志收集，将收集推送给Elasticsearch存储，由于Logstash比较重占用资源比较可观，所以我们使用轻量级的Filebeat在客户服务器上进行日志收集然后推送给Logstash，Logstash再推送给Elasticsearch存储，如果日志量比较大，可能会对FIlebeat到Logstash这个管道处理产生比较大负荷，所以我们可以在这部分的中间插入一个队列进行缓冲，比如redis或者kafka。 各个组件安装 接下来我们就在docker里安装各个用到的组件，由于我自己的电脑使用的ubuntu系统比较熟悉，所以我们还是使用ubuntu的环境，由于有几个组件是基于java开发的，所以运行需要java环境，需要jdk8以上，如何安装java环境这里就不操作了。 docker环境准备 我们首选需要一个docker的ubuntu的基础官方镜像 1$ docker search ubuntu 这个命令可以查询ubuntu相关的镜像文件，是按照星级排序的，OFFICIAL 一栏的[OK]表示是官方镜像，我们使用下面的命令进行镜像下载。 1$ docker pull ubuntu 启动ubuntu镜像12$ docker run -t -i ubuntu:latest /bin/bashroot@51a064df9e85:/# 我们新建一个用户，后面会用到1234567891011121314151617root@51a064df9e85:/# adduser tigerAdding user `tiger' ...Adding new group `tiger' (1000) ...Adding new user `tiger' (1000) with group `tiger' ...Creating home directory `/home/tiger' ...Copying files from `/etc/skel' ...Enter new UNIX password: Retype new UNIX password: passwd: password updated successfullyChanging the user information for tigerEnter the new value, or press ENTER for the default Full Name []: Room Number []: Work Phone []: Home Phone []: Other []: Is the information correct? [Y/n] y 我们已经运行镜像创建了ubuntu的容器，我们现在就在容器里安装各个组件。首先申明下，这个ubuntu是简化版可能很多命令没有包括在这个镜像内，需要自行安装，如何安装可以百度。 Elasticsearch安装 下载地址：https://www.elastic.co/downloads/elasticsearch， 现在最新的是6.1.1，图有点老了。 切换到刚才创建的用户，并且建立一个目录用来存放下载的文件，然后进入该目录进行下载1234root@51a064df9e85:/# su tigertiger@51a064df9e85:/$ mkdir ~/downloadstiger@51a064df9e85:/$ cd ~/downloads/tiger@51a064df9e85:~/downloads$ 1tiger@51a064df9e85:~/downloads$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.1.1.tar.gz 解压文件然后吧解压出来的文件夹移动到/opt/下，我一般喜欢把安装文件放到/opt/目录下1234tiger@51a064df9e85:~/downloads$ tar -zxvf elasticsearch-6.1.1.tar.gztiger@51a064df9e85:~/downloads$ sudo mv elasticsearch-6.1.1 /opt/tiger@51a064df9e85:~/downloads$ cd /opt/elasticsearch-6.1.1/tiger@46a32b71c85e:/opt/elasticsearch-6.1.1$ 进入到/opt/elasticsearch-6.1.1/目录进行配置文件修改1tiger@46a32b71c85e:/opt/elasticsearch-6.1.1$ vim ./config/elasticsearch.yml 找到network.host: 配置 取消注释 修改值为 0.0.0.0 然后就可以启动elasticsearch服务了，我这个是手动启动服务，要开机运行可以自己百度一下。1tiger@46a32b71c85e:/opt/elasticsearch-6.1.1$ ./bin/elasticsearch 打印一些日志后服务就启动好了，可以使用 curl ‘http://localhost:9200‘ 测试下服务是否正常。12345678910111213141516tiger@46a32b71c85e:/$ curl 'http://localhost:9200'&#123; "name" : "KdeFZ6P", "cluster_name" : "elasticsearch", "cluster_uuid" : "H4vGKoPnRPOxtywv-G6dkQ", "version" : &#123; "number" : "6.1.1", "build_hash" : "bd92e7f", "build_date" : "2017-12-17T20:23:25.338Z", "build_snapshot" : false, "lucene_version" : "7.1.0", "minimum_wire_compatibility_version" : "5.6.0", "minimum_index_compatibility_version" : "5.0.0" &#125;, "tagline" : "You Know, for Search"&#125; Kibana安装 下载地址：https://www.elastic.co/downloads/kibana 现在最新的是6.1.1，图有点老了。 按照Elasticsearch的下载安装步骤将解压缩出来的文件夹移动到/opt/目录下，然后进行Kibana的配置工作，主要是 server.host 以及 elasticsearch.url 的配置。 server.host 的值 如果我们使用nginx做代理那么 值可以是 localhost，如果需要在容器外部访问 设置为 0.0.0.0 elasticsearch.url的值设置为 elasticsearch的服务地址 由于我们是在同一个容器内默认的就可以 elasticsearch.url: http://localhost:9200进入kibana-6.1.1-linux-x86_64目录启动Kibana 1tiger@46a32b71c85e:/opt/kibana-6.1.1-linux-x86_64$ ./bin/kibana Kibana的访问地址是 http://localhost:5601 Logstash安装 下载地址：https://www.elastic.co/downloads/logstash 最新是6.1.1版本，按照Elasticsearch的下载安装步骤将解压缩出来的文件夹移动到/opt/目录下，然后进行Logstash的配置工作我们在/opt/logstash-6.1.1/config/目录下新建一个配置logstash-simple.conf，进行如下配置12345678910111213141516171819input &#123; beats &#123; port =&gt; "5044" &#125;&#125; filter &#123; grok &#123; match =&gt; &#123; "message" =&gt; "%&#123;COMBINEDAPACHELOG&#125;"&#125; &#125; geoip &#123; source =&gt; "clientip" &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [ "localhost:9200" ] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 由于我们是用Filebeat在客户服务器上启动收集日志的，所以我们开启5044端口监听，接受日志输入，具体配置文件的含义可以自行百度了解。 进入logstash-6.1.1目录启动Logstash1tiger@46a32b71c85e:/opt/logstash-6.1.1$ ./bin/logstash -f ./config/logstash-simple.conf Filebeat安装 Filebeat工具是安装到需要进行日志收集的机器上的，由于我的机器是ubuntu所以我下载ubuntu对应的版本，下载地址：https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.1.1-linux-x86_64.tar.gz 最新是6.1.1版本，解压到某个目录下，我的是放在/opt/filebeat-6.1.1-linux-x86_64/目录下的，然后进行filebeat的配置工作 打开配置文件/opt/filebeat-6.1.1-linux-x86_64/filebeat.yml，简单的配置如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849filebeat.prospectors:- type: log # Change to true to enable this prospector configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - c:\programdata\elasticsearch\logs\* # Filebeat以多快的频率去prospector指定的目录下面检测文件更新（比如是否有新增文件） # 如果设置为0s，则Filebeat会尽可能快地感知更新（占用的CPU会变高）。默认是10s scan_frequency: 100s # 设定Elasticsearch输出时的document的type字段 可以用来给日志进行分类。Default: log document_type: log ### Multiline options # Mutiline can be used for log messages spanning multiple lines. This is common # for Java Stack Traces or C-Line Continuation # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ multiline.pattern: ^\[ # Defines if the pattern set under pattern should be negated or not. Default is false. multiline.negate: true # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern # that was (not) matched before or after or as long as a pattern is not matched based on negate. # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash multiline.match: after # 合并的最多行数（包含匹配pattern的那一行） #max_lines: 500 # 如果设置为true，Filebeat从文件尾开始监控文件新增内容，把新增的每一行文件作为一个事件依次发送， # 而不是从文件开始处重新发送所有内容 tail_files: true #============================= Filebeat modules =============================== filebeat.config.modules: # Glob pattern for configuration loading #path: $&#123;path.config&#125;/modules.d/*.yml path: /opt/filebeat-6.1.1-linux-x86_64/filebeat.yml # Set to true to enable config reloading reload.enabled: true # Period on which files under path should be checked for changes #reload.period: 10s #----------------------------- Logstash output -------------------------------- output.logstash: # The Logstash hosts hosts: ["localhost:5044"] index: "hubei-solution-%&#123;+yyyy.MM.dd&#125;" # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"] # Certificate for SSL client authentication #ssl.certificate: "/etc/pki/client/cert.pem" # Client Certificate Key #ssl.key: "/etc/pki/client/cert.key" filebeat.prospectors.paths里的地址修改为你日志的地址，可以采用通配符*进行模糊匹配 multiline部分是对多行日志的设置，比如错误日志打印堆栈信息的就会有多行的情况，这个配置如何识别多行为一行 filebeat.config.modules.path设置全局配置文件的地址，可以设置你自己配置文件存放路径 我们输出到Logstash，所以我们开启Logstash output栏的配置，把其他输出设置注释掉，配置好Logstash的输入监听地址 进入Filebeat目录开启服务 -e参数会打印日志到前台1$ ./filebeat -e Nginx安装 我们使用ubuntu的 apt-get进行nginx的安装1$ sudo apt-get install nginx 使用openssl创建一个管理员（admin），这是用来登录Kibana web接口的：12$ sudo -v$ echo "admin:`openssl passwd -apr1`" | sudo tee -a /etc/nginx/htpasswd.users 按照提示设置admin用户的密码。编辑Nginx配置文件：1$ sudo vim /etc/nginx/sites-available/default 替换为下面的配置：1234567891011121314151617server &#123; listen 80; server_name your_domain_or_IP; auth_basic "Restricted Access"; auth_basic_user_file /etc/nginx/htpasswd.users; location / &#123; proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; &#125;&#125; 把your_domain_or_IP替换为你服务器的IP或域名；检查Nginx配置语法：1$ sudo nginx -t 指定配置文件进行启动Nginx：1$ nginx -c /etc/nginx/sites-available/default 开启相关端口映射 我们已经安装好了各个需要用到的组件，但是我们像上面启动各个服务只能在容器内部访问到，外面是不能访问的，所以我们需要开放需要让外部访问的端口，比如Logstash监听的5044端口，nginx配置监听的8080端口，或者如果希望外面能访问elasticsearch的restfull api 也可以开放配置的9200端口 docker 端口映射只需要在run 命令 带上 -p 9200:9200 就可以了 -p 表示映射指定的端口 -P 表示随机指定一个端口映射到docker容器 我们先关闭之前开启的容器，后面的一串字符串是你容器的ID1$ docker stop 9929c83b1f74 最好吧我们刚才修改的容器保存为镜像以免丢失，elk:ubuntu 是我取的镜像的名称1$ docker commit 9929c83b1f74 elk:ubuntu 重新运行镜像创建新容器，我们映射了三个端口1$ docker run -p 5044:5044 -p 8080:80 -p 9200:9200 -d elk:ubuntu /run.sh 现在我们访问下 http://localhost:8080 ，输入前面安装Nginx设置的用户名密码就可以看到Kibana的界面了 总结 目前我们只是简单的搭建了一个elk的环境，跑通了从采集日志到查询展示日志也就是从Filebeat到Kibana的各个环节，但是各个组件的配置还有很多需要了解学习，elasticsearch还有很多知识需要学习，以后也会把自己学习实践到的东西继续写出来 参考文章：https://es.xiaoleilu.com/index.htmlhttp://blog.topspeedsnail.com/archives/4825http://www.linuxidc.com/Linux/2017-09/147092p2.htmhttps://www.cnblogs.com/sunxucool/p/3799190.htmlhttp://blog.csdn.net/chengxuyuanyonghu/article/details/54378778http://m.blog.csdn.net/u012516166/article/details/74946823]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>elasticsearch</tag>
        <tag>elk</tag>
        <tag>kibana</tag>
        <tag>logstash</tag>
        <tag>filebeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取github中创建静态站点的用户信息]]></title>
    <url>%2F2017%2F12%2F17%2FGithubPageUsersAcquire%2F</url>
    <content type="text"><![CDATA[功能介绍 最近在[GitHub Pages]:(https://pages.github.com/) 上使用hexo创建了一个静态博客，觉得GitHub提供的这个服务蛮有意思的，所以突发奇想想看看到底有多少用户使用GItHub Pages创建了自己的静态站点，所以就写了个python的小爬虫想爬一爬GitHub的用户数据。 经过搜索了解到GitHub提供非常好用的[API]:(https://developer.github.com/v3/) ，可以非常方便的获取用户各种信息。 使用工具 操作系统 ubuntu16.04 语言python3 爬虫框架pyspider，该框架是国人开发的强大的开源爬虫框架 代理工具squid，由于github对同一个ip调用api的次数有限制，超过次数会被限制访问，所以需要代理访问。爬取思路获取用户个人信息REST API https://api.github.com/users/{username}通过传入用户登录名就可以获取到该用户的个人信息，例如调用https://api.github.com/users/tigerphz 获取到返回的json数据，下面仅列出重点数据 12345678910111213141516&#123;login: "tigerphz",//用户名id: 5802152,avatar_url: "https://avatars3.githubusercontent.com/u/5802152?v=4",url: "https://api.github.com/users/tigerphz",followers_url: "https://api.github.com/users/tigerphz/followers",//粉丝列表地址following_url: "https://api.github.com/users/tigerphz/following&#123;/other_user&#125;",//关注列表地址repos_url: "https://api.github.com/users/tigerphz/repos",//项目列表地址type: "User",site_admin: false,name: "tigerphz",public_repos: 20,//公共项目个数public_gists: 0,followers: 1,//粉丝人数following: 3//关注人数&#125; 获取项目信息REST API https://api.github.com/users/{username}/repos?page={page}通过该接口可以拿到用户的项目列表，通过上面获取到的public_repos: 20 表示该用户有20个公共项目，可以计算出需要请求的总页数，默认每页显示30个项目。下面紧列出重点数据123456789101112131415161718192021[&#123;id: 70320275,name: "aspnetboilerplate",full_name: "tigerphz/aspnetboilerplate",owner: &#123;login: "tigerphz",id: 5802152,avatar_url: "https://avatars3.githubusercontent.com/u/5802152?v=4",gravatar_id: "",url: "https://api.github.com/users/tigerphz",html_url: "https://github.com/tigerphz",followers_url: "https://api.github.com/users/tigerphz/followers",following_url: "https://api.github.com/users/tigerphz/following&#123;/other_user&#125;",type: "User",site_admin: false&#125;,private: false,html_url: "https://github.com/tigerphz/aspnetboilerplate"&#125;] name: “aspnetboilerplate” 就是项目名称，由于GItHub Pages的规定项目名称必须是username.github.io或者username.github.com格式，所以很容易的可以初步筛选出符合规定的项目 获取关注人列表REST API https://api.github.com/users/{username}/following?page={page}通过该接口可以拿到用户的关注人列表，通过上面获取到的following: 3 表示该用户有3关注人，可以计算出需要请求的总页数，默认每页显示30个项目。下面紧列出重点数据1234567891011121314151617[&#123;login: "JakeWharton",id: 66577,followers_url: "https://api.github.com/users/JakeWharton/followers",following_url: "https://api.github.com/users/JakeWharton/following&#123;/other_user&#125;",organizations_url: "https://api.github.com/users/JakeWharton/orgs",repos_url: "https://api.github.com/users/JakeWharton/repos"&#125;,&#123;login: "stone0090",id: 1546345,followers_url: "https://api.github.com/users/stone0090/followers",following_url: "https://api.github.com/users/stone0090/following&#123;/other_user&#125;",repos_url: "https://api.github.com/users/stone0090/repos"&#125;] 通过上面的信息我们可以拿到我们关注人的个人信息，进而可以获取到关注人的项目信息、关注人列表、粉丝列表 获取粉丝列表REST API https://api.github.com/users/{username}/followers?page={page}通过该接口可以拿到用户的关注人列表，通过上面获取到的followers: 1 表示该用户有1个粉丝，可以计算出需要请求的总页数，默认每页显示30个项目。123456789[&#123;login: "LowApe",id: 25314586,followers_url: "https://api.github.com/users/LowApe/followers",following_url: "https://api.github.com/users/LowApe/following&#123;/other_user&#125;"repos_url: "https://api.github.com/users/LowApe/repos"&#125;] 通过上面的信息我们可以拿到我们粉丝的个人信息，进而可以获取到粉丝的项目信息、关注人信息、粉丝列表 只需要这四个接口，我们就可以遍历整个用户关系网获取到非常多用户数据。 关键代码pyspider入口下面的代码是pyspider任务执行入口，会调用on_start方法开始执行任务，代码里有一些关键配置，比如代理ip设置，获取随机的通用请求头信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# -*- coding=utf8 -*-""" pyspider任务"""import loggingfrom pyspider.libs.base_handler import *from pyspider.libs.header_switch import HeadersSelectorfrom ..settings import BASE_USER_URL_LISTfrom . import user_relationship_analysis as analysisLOGGER = logging.getLogger(__name__)class Handler(BaseHandler): header_helper = HeadersSelector() header_helper.set_host(host='api.github.com') crawl_config = &#123; 'headers': header_helper.get_random_header(), 'timeout': 1000, 'proxy': '127.0.0.1:3128' &#125; def __init__(self): self.base_url_list = BASE_USER_URL_LIST @every(minutes=24 * 60) def on_start(self): for url in self.base_url_list: self.crawl(url, callback=self.analysis_user) @config(age=10 * 24 * 60 * 60) def analysis_user(self, response): user, follower_urls, following_urls, repo_urls = analysis.analysis_user( response) for repo_url in repo_urls: self.crawl(repo_url, callback=self.analysis_repo) for follower_url in follower_urls: self.crawl(follower_url, callback=self.analysis_follower) for following_url in following_urls: self.crawl(following_url, callback=self.analysis_following) @config(priority=2) def analysis_repo(self, response): analysis.analysis_repo(response) @config(age=10 * 24 * 60 * 60) def analysis_follower(self, response): user_urls = analysis.analysis_follower(response) for user_url in user_urls: self.crawl(user_url, callback=self.analysis_user) @config(age=10 * 24 * 60 * 60) def analysis_following(self, response): user_urls = analysis.analysis_following(response) for user_url in user_urls: self.crawl(user_url, callback=self.analysis_user) 用户关系解析下面的代码是进行上面介绍的四个接口地址生成使用的包括分页关键数据保存使用的redis123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# -*- coding=utf8 -*-""" 用户关系解析"""import loggingfrom ..extensions import redis_clientfrom .. import utilsLOGGER = logging.getLogger(__name__)def analysis_user(response): """解析用户 https://api.github.com/users/&#123;username&#125; :param response: :return: """ data = response.json user_id = data.get('login') if not user_id: return user = &#123;'id': user_id, 'type': data.get('type'), 'name': data.get('name'), 'company': data.get('company'), 'blog': data.get('blog'), 'location': data.get('location'), 'email': data.get('email'), 'repos_count': data.get('public_repos', 0), 'gists_count': data.get('public_gists', 0), 'followers': data.get('followers', 0), 'following': data.get('following', 0), 'created_at': data.get('created_at'), 'avatar_url': data.get('avatar_url')&#125; follower_urls = utils.get_url_list( user['id'], utils.get_user_follower_url, user['followers']) following_urls = utils.get_url_list( user['id'], utils.get_user_following_url, user['following']) repo_urls = utils.get_url_list( user['id'], utils.get_user_repo_url, user['repos_count']) redis_client.sadd('github_users', user_id) return user, follower_urls, following_urls, repo_urlsdef analysis_repo(response): """ 解析用户的项目 """ data = response.json for item in data: full_name = item['full_name'] user_name = item['owner']['login'] if not utils.check_repo_github_papers(user_name, full_name): continue redis_client.hset('github_papers', user_name, full_name)def analysis_follower(response): """ 解析用户的粉丝列表 """ data = response.json urls = [] for item in data: user_name = item['login'] urls.append(utils.get_user_page_url(user_name)) return urlsdef analysis_following(response): """ 解析用户关注列表 """ data = response.json urls = [] for item in data: user_name = item['login'] urls.append(utils.get_user_page_url(user_name)) return urls 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# -*- coding=utf8 -*-""" url地址"""from . import contsdef get_user_page_url(user_name): """ 获取用户主页url:https://api.github.com/users/&#123;user_name&#125;&#125; """ return 'https://&#123;&#125;/users/&#123;&#125;'.format(conts.GITHUB_API_HOST, user_name)def get_user_follower_url(user_name, page=1): """ 获取用户的粉丝列表rul：https://api.github.com/users/&#123;user_name&#125;/followers?page=&#123;page&#125; """ return 'https://&#123;&#125;/users/&#123;&#125;/followers?page=&#123;&#125;'.format(conts.GITHUB_API_HOST, user_name, page)def get_user_following_url(user_name, page=1): """ 获取用户关注的用户列表url：https://api.github.com/users/&#123;user_name&#125;/following?page=&#123;page&#125; """ return 'https://&#123;&#125;/users/&#123;&#125;/following?page=&#123;&#125;'.format(conts.GITHUB_API_HOST, user_name, page)def get_user_repo_url(user_name, page=1): """ 获取用户项目列表url:https://api.github.com/users/&#123;user_name&#125;/repos?page=&#123;page&#125; """ return 'https://&#123;&#125;/users/&#123;&#125;/repos?page=&#123;&#125;'.format(conts.GITHUB_API_HOST, user_name, page)def get_url_list(user_name, func, count): """ 根据总数调用func生成url """ result = [] page = 1 while (page - 1) * conts.PAGE_SIZE &lt; count: result.append(func(user_name, page)) page += 1 return resultdef check_repo_github_papers(user_name, repo_name): """ 判断项目是不是静态博客，&#123;user_name&#125;.github.com &#123;user_name&#125;.github.io Args: user_name(string) 用户名 repo_name(string) 项目全名 """ part = repo_name[len(user_name):] return '.github.com' in part or '.github.io' in part 获取到的部分用户信息1234567891011121314151617181920212223127.0.0.1:6379&gt; HGETALL github_papers 1) "tigerphz" 2) "tigerphz/tigerphz.github.com" 3) "LowApe" 4) "LowApe/didiaoyuan.github.io" 5) "stone0090" 6) "stone0090/stone0090.github.io" 7) "lfq7413" 8) "lfq7413/lfq7413.github.io" 9) "oguzcelik"10) "oguzcelik/oguzcelik.github.io"11) "astaxie"12) "astaxie/astaxie.github.io"13) "Trinea"14) "Trinea/trinea.github.com"15) "Slock"16) "Slock/Slock.github.io"17) "taoyuanxiaoqi"18) "taoyuanxiaoqi/taoyuanxiaoqi.github.io"19) "viti"20) "viti/viti.github.io"21) "kevinzh64"22) "kevinzh64/kevinzh64.github.io"]]></content>
      <categories>
        <category>python爬虫</category>
      </categories>
      <tags>
        <tag>github pages</tag>
        <tag>pyspider</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 安装]]></title>
    <url>%2F2017%2F12%2F11%2Fdocker%2F</url>
    <content type="text"><![CDATA[Docker 版本简介 Docker是基于Go语言实现的开源容器项目，诞生于2013年初，最初发起者是dotCloud公司。Docker自开源后收到广泛的关注和讨论，逐渐形成了围绕Docker的生态圈，dotCloud公司后来改名Dockker Inc，专注于Docker相关的技术和产品。Docker的主要目标是’Build,Ship and Run Any App,Anywhere’，即通过对应用组件的封装、分发、不熟、运行等生命周期的管理，达到应用组件级别的“一次封装，到处运行”，Docker基于linux的多项开源技术提供高效、敏捷和轻量级的容器方案。 目前Docker分为了两个版本Community Edition (CE) 和 Enterprise Edition (EE)社区版（CE）是免费的，面向开发者或者小团队使用的，用户可以选择stable(更新发布较慢) 和 edge(每个月都会发布新特性) 两个版本。 企业版(EE)是付费使用的 社区版（CE） Docker CE是免费的Docker产品的新名字，是为开发人员或者小团队创建基于容器的应用，Docker CE有两个更新版本 Stable 每季度发布，适用于希望追求稳定的用户，提供4个月支持 Edge 每个月发布，主要面向喜欢尝试新功能的用户，提供一个月支持 企业版(EE)Docker EE是专门为企业的发展和IT团队创建的，Docker EE为企业提供最安全的容器平台，Docker EE提供三个服务层次： 服务层级 功能 Base 包含用于认证基础设施的Docker平台Docker公司的支持经过 认证的、来自Docker Store的容器与插件 Standard 添加高级镜像与容器管理LDAP/AD用户集成基于角色的访问控制(Docker Datacenter) Advanced 添加Docker安全扫描连续漏洞监控 每个Docker EE版本都享受为期一年的支持跟维护期，在此期间接受安全与关键问题修正。 Docker CE 版本安装本次基于ubuntu系统安装，安装Docker CE, 你需要下面列出的64位的Ubuntu版本的一个 Artful 17.10 (Docker CE 17.11 Edge only) Zesty 17.04 Xenial 16.04 (LTS) Trusty 14.04 (LTS)Docker CE 在Ubutnu中支持基于x86_64, armhf, 和 s390x (IBM z Systems)的架构检查kernel版本和操作系统架构12345678910$ uname -aLinux tigerphz-pc 4.10.0-42-generic #46~16.04.1-Ubuntu SMP Mon Dec 4 15:57:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux$ cat /proc/version Linux version 4.10.0-42-generic (buildd@lgw01-amd64-007) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) ) #46~16.04.1-Ubuntu SMP Mon Dec 4 15:57:59 UTC 2017$ lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 16.04.3 LTSRelease: 16.04Codename: xenial 可以看到我们使用的是代号xenial的ubuntu 16.04 64位版本和kernel4.10内核 安装Docker CE更新apt源1$ sudo apt-get update 安装下面的包使得支持https12345$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common 添加Docker官方GPG秘钥1$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 添加系统版本对应的源 amd64 1234$ sudo add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable" armhf 1234$ sudo add-apt-repository \ "deb [arch=armhf] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable" s390x 1234$ sudo add-apt-repository \ "deb [arch=s390x] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable" 更新apt源1$ sudo apt-get update 安装 docker-ce1$ sudo apt-get install docker-ce 等待安装，安装好以后可以查看版本1234567891011121314151617$ docker version Client: Version: 17.09.1-ce API version: 1.32 Go version: go1.8.3 Git commit: 19e2cf6 Built: Thu Dec 7 22:24:23 2017 OS/Arch: linux/amd64Server: Version: 17.09.1-ce API version: 1.32 (minimum version 1.12) Go version: go1.8.3 Git commit: 19e2cf6 Built: Thu Dec 7 22:23:00 2017 OS/Arch: linux/amd64 Experimental: false 还可以输入一下命令，测试运行下是否成功安装好1$ sudo docker run hello-world 该命令会去 Docker默认的远程仓库下载名为hello-world的镜像文件到本地，然后运行输出一段信息。1234567891011121314151617181920212223$ sudo docker run hello-world [sudo] tiger 的密码： Hello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the "hello-world" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://cloud.docker.com/For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 每次运行docker需要带上sudo，比较麻烦，可以创建一个docker组然后把当前用户加入改组 12$ sudo groupadd docker$ sudo usermod -aG docker tiger 查看本地下载的镜像文件，发现了hello-world123$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest f2a91732366c 2 weeks ago 1.85kB]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F404.html</url>
    <content type="text"><![CDATA[&lt;!DOCTYPE HTML&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[about]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[categories]]></title>
    <url>%2Fcategories%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[tags]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
