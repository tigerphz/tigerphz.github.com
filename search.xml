<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[docker安装redis带自定义配置]]></title>
    <url>%2F2018%2F05%2F17%2Fdocker-redis-install%2F</url>
    <content type="text"><![CDATA[redis官方镜像进入从官方的redis镜像页面，下面有内容教你如何使用镜像 How to use this imagestart a redis instance$ docker run –name some-redis -d redisThis image includes EXPOSE 6379 (the redis port), so standard container linking will make it automatically available to the linked containers (as the following examples illustrate). start with persistent storage$ docker run –name some-redis -d redis redis-server –appendonly yesIf persistence is enabled, data is stored in the VOLUME /data, which can be used with –volumes-from some-volume-container or -v /docker/host/dir:/data (see docs.docker volumes). For more about Redis Persistence, see http://redis.io/topics/persistence. connect to it from an application$ docker run –name some-app –link some-redis:redis -d application-that-uses-redis… or via redis-cli$ docker run -it –link some-redis:redis –rm redis redis-cli -h redis -p 6379Additionally, If you want to use your own redis.conf …You can create your own Dockerfile that adds a redis.conf from the context into /data/, like so. FROM redisCOPY redis.conf /usr/local/etc/redis/redis.confCMD [ “redis-server”, “/usr/local/etc/redis/redis.conf” ]Alternatively, you can specify something along the same lines with docker run options. $ docker run -v /myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf –name myredis redis redis-server /usr/local/etc/redis/redis.confWhere /myredis/conf/ is a local directory containing your redis.conf file. Using this method means that there is no need for you to have a Dockerfile for your redis container. 上面介绍了几种用法： 直接docker run redis，使用默认配置 配置数据持久化 –appendonly yes 映射数据目录到宿主机 -v /docker/host/dir:/data 使用自定义配置文件启动redis -v /myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf 运行命令1docker run --restart=on-failure:10 -p 6379:6379 --name redis -v $PWD/redis/redis.conf:/usr/local/etc/redis/redis.conf -v $PWD/redis/data:/data -d redis redis-server /usr/local/etc/redis/redis.conf --appendonly yes 命令解释： –restart=on-failure:10 表示只有在非0状态退出时才从新启动容器，Docker将尝试重新启动容器的最多10次 -p 6379:6379 映射宿主机端口 –name redis 自定义名字 -v $PWD/redis/redis.conf:/usr/local/etc/redis/redis.conf 容器内文件与宿主机文件映射，$PWD表示当前登陆用户home路径 -v $PWD/redis/data:/data 容器内文件夹与宿主机文件夹映射，redis保存文件路径是/data -d redis redis-server /usr/local/etc/redis/redis.conf 使用/usr/local/etc/redis/redis.conf路径的配置文件启动redis，该配置以及映射到我们自定义的$PWD/redis/redis.conf配置文件上 –appendonly yes 配置数据持久化 –restart具体参数值详细信息 no - 容器退出时，不重启容器； on-failure - 只有在非0状态退出时才从新启动容器； always - 无论退出状态是如何，都重启容器； 注意事项首先上面的命名运行需要我们有自定义的配置文件,redis镜像是不带配置文件的，我们可以从redis官网下载redis包找到里面的redis.conf放到$PWD/redis/路径下。如果使用默认的配置那么启动redis容器后无法在宿主机上直接访问redis，因为默认配置文件里配置的访问ip是：1bind 127.0.0.1 我们需要修改为1bind 0.0.0.0 其他配置项按需修改。 连接方式12docker exec -it 87e2a4f1c007 redis-cliredis-cli -a redis -h 192.168.164.130 -p 6379 配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option &quot;include&quot; won&apos;t be rewritten by command &quot;CONFIG REWRITE&quot;# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you&apos;d better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 lookback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#bind 127.0.0.1 bind 0.0.0.0# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# &quot;bind&quot; directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the &quot;bind&quot; directive.protected-mode yes# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# GENERAL ###################################### By default Redis does not run as a daemon. Use &apos;yes&apos; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;&quot;# To enable logging to the system logger, just set &apos;syslog-enabled&apos; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &apos;databases&apos;-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all &quot;save&quot; lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that&apos;s set to &apos;yes&apos; as it&apos;s almost always a win.# If you want to save some CPU in the saving child set it to &apos;no&apos; but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &apos;dbfilename&apos; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Slave replication. Use slaveof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of slaves.# 2) Redis slaves are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition slaves automatically try to reconnect to masters# and resynchronize with them.## slaveof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the slave to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the slave request.## masterauth &lt;master-password&gt;# When a slave loses its connection with the master, or when the replication# is still in progress, the slave can act in two different ways:## 1) if slave-serve-stale-data is set to &apos;yes&apos; (the default) the slave will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if slave-serve-stale-data is set to &apos;no&apos; the slave will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO and SLAVEOF.#slave-serve-stale-data yes# You can configure a slave instance to accept writes or not. Writing against# a slave instance may be useful to store some ephemeral data (because data# written on a slave will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default slaves are read-only.## Note: read only slaves are not designed to be exposed to untrusted clients# on the internet. It&apos;s just a protection layer against misuse of the instance.# Still a read only slave exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only slaves using &apos;rename-command&apos; to shadow all the# administrative / dangerous commands.slave-read-only yes# Replication SYNC strategy: disk or socket.## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## New slaves and reconnecting slaves that are not able to continue the replication# process just receiving differences, need to do what is called a &quot;full# synchronization&quot;. An RDB file is transmitted from the master to the slaves.# The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the slaves incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to slave sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more slaves# can be queued and served with the RDB file as soon as the current child producing# the RDB file finishes its work. With diskless replication instead once# the transfer starts, new slaves arriving will be queued and a new transfer# will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple slaves# will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the slaves.## This is important since once the transfer starts, it is not possible to serve# new slaves arriving, that will be queued for the next RDB transfer, so the server# waits a delay in order to let more slaves arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# Slaves send PINGs to server in a predefined interval. It&apos;s possible to change# this interval with the repl_ping_slave_period option. The default value is 10# seconds.## repl-ping-slave-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of slave.# 2) Master timeout from the point of view of slaves (data, pings).# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-slave-period otherwise a timeout will be detected# every time there is low traffic between the master and the slave.## repl-timeout 60# Disable TCP_NODELAY on the slave socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to slaves. But this can add a delay for# the data to appear on the slave side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the slave side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and slaves are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# slave data when slaves are disconnected for some time, so that when a slave# wants to reconnect again, often a full resync is not needed, but a partial# resync is enough, just passing the portion of data the slave missed while# disconnected.## The bigger the replication backlog, the longer the time the slave can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a slave connected.## repl-backlog-size 1mb# After a master has no longer connected slaves for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last slave disconnected, for# the backlog buffer to be freed.## Note that slaves never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly &quot;partially# resynchronize&quot; with the slaves: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The slave priority is an integer number published by Redis in the INFO output.# It is used by Redis Sentinel in order to select a slave to promote into a# master if the master is no longer working correctly.## A slave with a low priority number is considered better for promotion, so# for instance if there are three slaves with priority 10, 100, 25 Sentinel will# pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the slave as not able to perform the# role of master, so a slave with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.slave-priority 100# It is possible for a master to stop accepting writes if there are less than# N slaves connected, having a lag less or equal than M seconds.## The N slaves need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the slave, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough slaves# are available, to the specified number of seconds.## For example to require at least 3 slaves with a lag &lt;= 10 seconds use:## min-slaves-to-write 3# min-slaves-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-slaves-to-write is set to 0 (feature disabled) and# min-slaves-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# slaves in different ways. For example the &quot;INFO replication&quot; section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover slave instances.# Another place where this info is available is in the output of the# &quot;ROLE&quot; command of a master.## The listed IP and address normally reported by a slave is obtained# in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the slave to connect with the master.## Port: The port is communicated by the slave during the replication# handshake, and is normally the port that the slave is using to# list for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the slave may be actually reachable via different IP and port# pairs. The following two options can be used by a slave in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## slave-announce-ip 5.5.5.5# slave-announce-port 1234################################## SECURITY #################################### Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other# commands. This might be useful in environments in which you do not trust# others with access to the host running redis-server.## This should stay commented out for backward compatibility and because most# people do not need auth (e.g. they run their own servers).## Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## requirepass foobared# Command renaming.## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to slaves may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error &apos;max number of clients reached&apos;.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can&apos;t remove keys according to the policy, or if the policy is# set to &apos;noeviction&apos;, Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the &apos;noeviction&apos; policy).## WARNING: If you have slaves attached to an instance with maxmemory on,# the size of the output buffers needed to feed the slaves are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of slaves is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have slaves attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for slave# output buffers (but this is not needed if the policy is &apos;noeviction&apos;).## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select among five behaviors:## volatile-lru -&gt; Evict using approximated LRU among the keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU among the keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key among the ones with an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don&apos;t evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It&apos;s up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a slave performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transfered.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives:lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noslave-lazy-flush no############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don&apos;t fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is &quot;everysec&quot;, as that&apos;s usually the right compromise between# speed and data safety. It&apos;s up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that&apos;s snapshotting),# or on the contrary, use &quot;always&quot; that&apos;s very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it&apos;s possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can&apos;t happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:## [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;# string and loads the prefixed RDB file, and continues loading the AOF# tail.## This is currently turned off by default in order to avoid the surprise# of a format change, but will at some point be used as the default.aof-use-rdb-preamble no################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn&apos;t want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however# in order to mark it as &quot;mature&quot; we need to wait for a non trivial percentage# of users to deploy it in production.# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++## Normal Redis instances can&apos;t be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A slave of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a slave to actually have an exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple slaves able to failover, they exchange messages# in order to try to give an advantage to the slave with the best# replication offset (more data from the master processed).# Slaves will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single slave computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the &quot;connected&quot; state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the slave will not try to failover# at all.## The point &quot;2&quot; can be tuned by user. Specifically a slave will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * slave-validity-factor) + repl-ping-slave-period## So for example if node-timeout is 30 seconds, and the slave-validity-factor# is 10, and assuming a default repl-ping-slave-period of 10 seconds, the# slave will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large slave-validity-factor may allow slaves with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a slave at all.## For maximum availability, it is possible to set the slave-validity-factor# to a value of 0, which means, that slaves will always try to failover the# master regardless of the last time they interacted with the master.# (However they&apos;ll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-slave-validity-factor 10# Cluster slaves are able to migrate to orphaned masters, that are masters# that are left without working slaves. This improves the cluster ability# to resist to failures as otherwise an orphaned master can&apos;t be failed over# in case of failure if it has no working slaves.## Slaves migrate to orphaned masters only if there are still at least a# given number of other working slaves for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a slave# will migrate only if there is at least 1 other working slave for its master# and so forth. It usually reflects the number of slaves you want for every# master in your cluster.## Default is 1 (slaves migrate only if their masters remain with at least# one slave). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents slaves from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-slave-no-failover no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don&apos;t have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# A Alias for g$lshzxe, so that the &quot;AKE&quot; string means all the events.## The &quot;notify-keyspace-events&quot; takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don&apos;t need# this feature and the feature has some overhead. Note that if you don&apos;t# specify at least one of K or E, no events will be delivered.notify-keyspace-events &quot;&quot;############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don&apos;t start compressing until after 1 node into the list,# going from either the head or tail&quot;# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don&apos;t compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don&apos;t have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can&apos;t consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# slave -&gt; slave clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don&apos;t receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and slave clients, since# subscribers and slaves receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here.## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified &quot;hz&quot; value.## By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it&apos;s maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested# even in production and manually tested by multiple engineers for some# time.## What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an &quot;hot&quot; way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis# to use the copy of Jemalloc we ship with the source code of Redis.# This is the default with Linux builds.## 2. You never need to enable this feature if you don&apos;t have fragmentation# issues.## 3. Once you experience fragmentation, you can enable this feature when# needed with the command &quot;CONFIG SET activedefrag yes&quot;.## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag yes# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage# active-defrag-cycle-min 25# Maximal effort for defrag in CPU percentage# active-defrag-cycle-max 75]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu服务器断电或异常重启,无法正常进入系统]]></title>
    <url>%2F2018%2F05%2F13%2Flinux-ubuntu-fsck%2F</url>
    <content type="text"><![CDATA[问题描述本人在两台ubuntu16.04的VM虚拟机上部署了rancher与k8s,运行一段时间后无法连接rancher容器，后尝试关闭重启容器也失败，只得重启服务器，然后服务器却无法正常进入登陆界面，如图：该界面提示了异常原因，应该是异常重启导致分区有问题，需要检测修复下分区。 fsck命令运行fsck对有问题的分区进行修复，按照提示一路yes就可以1$ fsck -t ext4 /dev/mapper/hadoop--vg-root -t ext4 指定文件系统类型 /devmapper/hadoop–vg-root 要修复的分区路径，按照各自实际情况 提示完成后reboot 重启下服务器就可以了]]></content>
      <categories>
        <category>linux常见问题</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ubuntu</tag>
        <tag>fsck</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker搭建私人仓库]]></title>
    <url>%2F2018%2F05%2F12%2Fdocker-registry%2F</url>
    <content type="text"><![CDATA[docker 版本每个版本配置可能不尽相同，我使用的docker是18.0312[22:27:38] hadoop:~ $ docker -vDocker version 18.03.0-ce, build 0520e24 下载docker registry镜像我们首先拉去docker registry私人仓库的docker镜像文件1[22:41:45] hadoop:~ $ sudo docker pull registry 运行registry容器1[22:43:48] hadoop:~ $ sudo docker run -d -p 5000:5000 --name registry --restart=always -v /home/hadoop/docker_registry_data:/var/lib/registry registry -p 5000:5000 表示我们映射宿主机的5000端口到容器的5000端口，使得宿主机可以访问registry容器 -v /home/hadoop/docker_registry_data:/var/lib/registry 表示把宿主机的 /home/hadoop/docker_registry_data目录(根据个人情况修改)映射到容器的/var/lib/registry目录，使得registry上传的镜像文件能保存到宿主机的文件系统上。 –restart=always 表示重启的策略，假如这个容器异常退出会自动重启容器； –name registry 进行重命名 镜像仓库配置镜像仓库内镜像的命名规范：[registry-name]/[namespace]/[imagename]:[version]为了解决https的报错需要配置daemon.json的insecure-registries，因为docker默认使用https协议，但是本地现在只支持http协议，配置insecure-registries：1234567[22:44:14] hadoop:~ $ sudo vim /etc/docker/daemon.json&#123; "registry-mirrors": ["配置自己的加速器地址如阿里云"], "insecure-registries": [ "192.168.164.133:5000" ]&#125; “registry-mirrors”: [“配置自己的加速器地址如阿里云”], 这语句如果不想配置可以去掉。重新启动docker使得配置生效1[23:07:30] hadoop:~ $ sudo systemctl restart docker 测试上传我们先拉取一个镜像,然后修改名字使得符合命名规范12[22:57:20] hadoop:~ $ sudo docker pull busybox[23:01:12] hadoop:~ $ sudo docker tag busybox 192.168.164.133:5000/penghuzi/busybox_test:v1.0 192.168.164.133为自己私人仓库地址测试上传该镜像到我们的私人仓库中1234[22:25:19] hadoop:~ $ sudo docker push 192.168.164.133:5000/penghuzi/busybox_test:v1.0The push refers to repository [192.168.164.133:5000/penghuzi/busybox_test]0314be9edf00: Pushedv1.0: digest: sha256:186694df7e479d2b8bf075d9e1b1d7a884c6de60470006d572350573bfa6dcd2 size: 527 查看私人仓库内镜像，以及指定镜像的tags1234[23:03:14] hadoop:~ $ curl 192.168.164.133:5000/v2/_catalog&#123;"repositories":["penghuzi/busybox_test"]&#125;[23:19:04] hadoop:~ $ curl 192.168.164.133:5000/v2/penghuzi/busybox_test/tags/list&#123;"name":"penghuzi/busybox_test","tags":["v1.0"]&#125; 下载私人仓库内的镜像1234[22:26:24] hadoop:~ $ sudo docker pull 192.168.164.133:5000/penghuzi/busybox_test:v1.0v1.0: Pulling from penghuzi/busybox_testDigest: sha256:186694df7e479d2b8bf075d9e1b1d7a884c6de60470006d572350573bfa6dcd2Status: Downloaded newer image for 192.168.164.133:5000/penghuzi/busybox_test:v1.0]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker 镜像仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-主题路由]]></title>
    <url>%2F2018%2F05%2F05%2Frabbitmq-topicrouting%2F</url>
    <content type="text"><![CDATA[前言上一节我们介绍了使用routingkey进行消息路由，通过使用direct类型的Exchange转发器与rout_key参数来做到匹配推送。如上图所示，我们使用的orange，green，black都属于颜色类别，如果我们衍生一点，假定我们现在要做一个关于动物信息收集的日志系统，动物属性有三类 行为：lazy，lively，quietness 颜色：orange，green，black 类型：rabbit，tiger，bird 我们规定使用[行为.颜色.类型]中间使用点符合分割，来表示某种类型的信息，现在Producer可以组合这三类表示一种信息类型进行发送比如lazy.orange.rabbit或者lazy.green.tiger等等，那么我们的queue又该如何操作才能接收到它关心的数据呢？这三类属性自由组合的数量多达3*3*3种，如果我们还是沿用上一节的Binding方式，设置好全部我们关心的消息类型显然是很麻烦的。假设我们的queue关心动物属性是颜色和行为的消息，难道要我们写很多次的queue_bind，把全部关心的消息类型都binding一次？ Topic Exchange对于上面的问题，我们需要一个更加负责的转发器就是Topic Exchange主题转发器topic类型的Exchagne可以放置任何的值作为routing_key，但是长度以及格式上有所限制，必须是’.’符号分割，比如上面的lazy.orange.rabbit或者lazy.green.tiger,还有长度不能超过225bytes。routing_key可以使用两个特殊字符进行模糊匹配 * (星号) 代表任意 一个单词 # (hash) 0个或者多个单词 现在我们使用topic类型的Exchange来完成我们上面提出的复杂消息的推送接收的问题。Producer发送消息依然需要设置routing_key，对于行为，颜色，类型这三类动物属性，我们的routing_key可以是类似lazy.orange.rabbit或者lazy.green.tiger这样的格式。我们使用两个queue队列与Exchange进行绑定 ，Q1的routing_key是”*.orange.*“，Q2的routing_key是“*.*.rabbit”与“lazy.#” Q1对所有orange颜色的动物感兴趣 Q2对所有的rabbits动物和所有的lazy行为感兴趣 举例： 如果发送消息时填写的routing_key是lazy.orange.rabbit，那么exchange会把该消息转发给Q1,Q2 如果发送消息时填写的routing_key是lazy.green.tiger，那么exchange会把消息转发给Q2 如果发送消息时填写的routing_key是quietness.orange.tiger，那么exchange会把消息转发给Q1 如果发送消息时填写的routing_key是quietness.black.rabbit，那么exchange会把消息转发给Q2，可以看到两个binding都匹配到了，但是也只会发送一次 如果发送消息时填写的routing_key是black.rabbit，那么exchange找不到与之binding的queue队列，因为这个routing_key只写了两个单词，没有与之匹配的binding的routing_key存在，除非写成lazy.orange或者lazy，那么可以匹配到’lazy.#‘也就是Q2,因为#可以匹配到零个或者多个单词 完整实例Producer：123456789101112131415161718192021222324252627282930#!/usr/bin/env pythonimport sysimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='topic_logs', exchange_type='topic')behaviors = ['lazy', 'lively', 'quietness']colors = ['orange', 'green', 'black']types = ['rabbit', 'tiger', 'bird']index = 0for b in behaviors: for c in colors: for t in types: severity = '%s.%s.%s' % (b, c, t) message = 'send logs routing_key=%s' % (severity,) channel.basic_publish(exchange='topic_logs', routing_key=severity, body=message, properties=pika.BasicProperties(delivery_mode=2)) print(" [x] Sent %r" % (message,)) index += 1print("send message num is %d" % (index,))connection.close() Consumer binding Q112345678910111213141516171819202122232425262728293031#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='topic_logs', exchange_type='topic')result = channel.queue_declare(exclusive=True)queuename = result.method.queuechannel.queue_bind(exchange='topic_logs', queue=queuename, routing_key='*.orange.*')print(' [*] Q1 Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(1) print('[x] done') ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue=queuename)channel.start_consuming() Consumer binding Q11234567891011121314151617181920212223242526272829303132#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='topic_logs', exchange_type='topic')result = channel.queue_declare(exclusive=True)queuename = result.method.queuechannel.queue_bind(exchange='topic_logs', queue=queuename, routing_key='*.*.rabbit')channel.queue_bind(exchange='topic_logs', queue=queuename, routing_key='lazy.#')print(' [*] Q2 Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(1) print('[x] done') ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue=queuename)channel.start_consuming() 打印日志Producer：123456789101112131415161718192021222324252627282930 [x] Sent 'send logs routing_key=lazy.orange.rabbit' [x] Sent 'send logs routing_key=lazy.orange.tiger' [x] Sent 'send logs routing_key=lazy.orange.bird' [x] Sent 'send logs routing_key=lazy.green.rabbit' [x] Sent 'send logs routing_key=lazy.green.tiger' [x] Sent 'send logs routing_key=lazy.green.bird' [x] Sent 'send logs routing_key=lazy.black.rabbit' [x] Sent 'send logs routing_key=lazy.black.tiger' [x] Sent 'send logs routing_key=lazy.black.bird' [x] Sent 'send logs routing_key=lively.orange.rabbit' [x] Sent 'send logs routing_key=lively.orange.tiger' [x] Sent 'send logs routing_key=lively.orange.bird' [x] Sent 'send logs routing_key=lively.green.rabbit' [x] Sent 'send logs routing_key=lively.green.tiger' [x] Sent 'send logs routing_key=lively.green.bird' [x] Sent 'send logs routing_key=lively.black.rabbit' [x] Sent 'send logs routing_key=lively.black.tiger' [x] Sent 'send logs routing_key=lively.black.bird' [x] Sent 'send logs routing_key=quietness.orange.rabbit' [x] Sent 'send logs routing_key=quietness.orange.tiger' [x] Sent 'send logs routing_key=quietness.orange.bird' [x] Sent 'send logs routing_key=quietness.green.rabbit' [x] Sent 'send logs routing_key=quietness.green.tiger' [x] Sent 'send logs routing_key=quietness.green.bird' [x] Sent 'send logs routing_key=quietness.black.rabbit' [x] Sent 'send logs routing_key=quietness.black.tiger' [x] Sent 'send logs routing_key=quietness.black.bird'send message num is 27Process finished with exit code 0 Consumer binding Q112345678910111213141516171819[*] Q1 Waiting for messages. To exit press CTRL+C[x] receive b'send logs routing_key=lazy.orange.rabbit'[x] done[x] receive b'send logs routing_key=lazy.orange.tiger'[x] done[x] receive b'send logs routing_key=lazy.orange.bird'[x] done[x] receive b'send logs routing_key=lively.orange.rabbit'[x] done[x] receive b'send logs routing_key=lively.orange.tiger'[x] done[x] receive b'send logs routing_key=lively.orange.bird'[x] done[x] receive b'send logs routing_key=quietness.orange.rabbit'[x] done[x] receive b'send logs routing_key=quietness.orange.tiger'[x] done[x] receive b'send logs routing_key=quietness.orange.bird'[x] done Consumer binding Q212345678910111213141516171819202122232425262728293031[*] Q2 Waiting for messages. To exit press CTRL+C[x] receive b'send logs routing_key=lazy.orange.rabbit'[x] done[x] receive b'send logs routing_key=lazy.orange.tiger'[x] done[x] receive b'send logs routing_key=lazy.orange.bird'[x] done[x] receive b'send logs routing_key=lazy.green.rabbit'[x] done[x] receive b'send logs routing_key=lazy.green.tiger'[x] done[x] receive b'send logs routing_key=lazy.green.bird'[x] done[x] receive b'send logs routing_key=lazy.black.rabbit'[x] done[x] receive b'send logs routing_key=lazy.black.tiger'[x] done[x] receive b'send logs routing_key=lazy.black.bird'[x] done[x] receive b'send logs routing_key=lively.orange.rabbit'[x] done[x] receive b'send logs routing_key=lively.green.rabbit'[x] done[x] receive b'send logs routing_key=lively.black.rabbit'[x] done[x] receive b'send logs routing_key=quietness.orange.rabbit'[x] done[x] receive b'send logs routing_key=quietness.green.rabbit'[x] done[x] receive b'send logs routing_key=quietness.black.rabbit'[x] done]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-routingkey路由]]></title>
    <url>%2F2018%2F05%2F03%2Frabbitmq-directrouting%2F</url>
    <content type="text"><![CDATA[前言上一节我们介绍了发布订阅模式使用Binding把Exchange转发器与queue队列联系起来了，使得Exchange知道要把消息发送到那些queue中，但是Exchange会把所有的消息都发送到与之关联的queue队列中，如果我们的queue只想接收跟它绑定的Exchange的某一类消息，能否实现呢？ Direct Bindings routing key上一节中我们我们介绍的Binding是这样写的1channel.queue_bind(exchange='direct_logs', queue=queuename) 其实后面还可以增加一个参数 routing_key1channel.queue_bind(exchange='direct_logs', queue=queuename, routing_key='orange') 这样与名字叫ldirect_logs的Exchange转发器绑定的该queue队列，现在只会接收到orange类型的消息了，Exchange转发器发送消息的时候会根据发送消息时填的routing_key的值找到完全比配的Binding是填写的routing_key的值的队列，然后把消息发送到这些队列中去。routing_key参数只对direct与topic类型的Exchange适用，fanout类型的Exchange使用routing_key不起作用。如何我们可以看出类型为direct的Exchange X与Q1,Q2队列绑定了，Q1的routing_key是 orange，Q2的routing_key是black,green两类消息，当Producer推送消息的routing_key是orange的时候，Exchagne X会把消息推送给Q1,如果Producer推送消息的routing_key是black或者green时会把消息推送给Q2，如果不是orange，black，green那么消息就会丢弃不会推送给这两个queue。同一个队列如何指定多个不同的routing_key呢？12channel.queue_bind(exchange='direct_logs', queue=queuename, routing_key='green')channel.queue_bind(exchange='direct_logs', queue=queuename, routing_key='black') queue_bind的时候指定不同的routing_key就可以了。 Multiple Bindings routing key多个queue使用相同的routing_key继续绑定也时可以的,如图类型为direct的Exchagne X与Q1,Q2队列绑定了，这两个队列的routing_key都时black,所以当Producer推送的消息的routing_key是black的时候，会同时推送给Q1,Q2队列，其他routing_key不是black的消息会丢弃掉。 Publish Message发送消息的时候跟之前的代码类似，只是我们需要多加一个routing_key参数1234#定义direct类型的exchagnechannel.exchange_declare(exchange='direct_logs', exchange_type='direct')#发送消息的时候指定routing_keychannel.basic_publish(exchange='direct_logs', routing_key='orange', body=message) 完整实例Producer：12345678910111213141516171819202122232425262728#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='direct_logs', exchange_type='direct')for num in range(1, 7): time.sleep(2) if num &lt;= 2: severity = 'orange' elif num &lt;= 4: severity = 'black' else: severity = 'green' message = 'send logs routing_key=%s' % (severity,) channel.basic_publish(exchange='direct_logs', routing_key=severity, body=message, properties=pika.BasicProperties(delivery_mode=2)) print(" [x] Sent %r" % (message,))connection.close() Consumer1 routing_key为orange12345678910111213141516171819202122232425262728293031#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='direct_logs', exchange_type='direct')result = channel.queue_declare(exclusive=True)queuename = result.method.queuechannel.queue_bind(exchange='direct_logs', queue=queuename, routing_key='orange')print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(2) print('[x] done') ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue=queuename)channel.start_consuming() Consumer2 routing_keyi为green和black1234567891011121314151617181920212223242526272829303132#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='direct_logs', exchange_type='direct')result = channel.queue_declare(exclusive=True)queuename = result.method.queuechannel.queue_bind(exchange='direct_logs', queue=queuename, routing_key='green')channel.queue_bind(exchange='direct_logs', queue=queuename, routing_key='black')print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(2) print('[x] done') ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue=queuename)channel.start_consuming() 打印的日志： 通过管理界面查看Exchange与队列的Bindings关系]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-发布/订阅]]></title>
    <url>%2F2018%2F05%2F01%2Frabbitmq-publishSubscribe%2F</url>
    <content type="text"><![CDATA[前言不知道大家注意到没，前面的文章我们发送消息的时候都是这么写的12channel.queue_declare(queue='cache_hello')channel.basic_publish(exchange='', routing_key='cache_hello', body=message) 关注第一个和第二个参数，第一个参数exchange我们赋值的是空字符串，第二个参数outing_key我们赋值的’cache_hello’，这个是我们定义的queue的名字。当我们给exchange赋值空字符串的时候，表示使用RabbitMQ Server的默认Exchange转发器，消息会根据我们指定的routing_key推送到与该值对应名字的消息队列中。我们可以看看管理页面里RabbitMQ Server定义的默认Exchange的描述默认的消息队列是支持持久化的并且是Direct类型的转发器。Bindings栏的英文表示默认Exchange转发器可以隐式绑定到与routing_key的值相同名字的队列上，默认Exchange转发器不能进行显示绑定或者解绑操作，也不能进行删除操作。我们之前的实例代码都是把消息推送到一个指定的queue队列中，接下来我们尝试使用“发布/订阅”把消息推送给多个队列，我们构建一个简单的日志系统来验证这种模式，Producer负责收集系统日志然后发出日志，Consumer负责接受日志并且打印日志，我们将构建两个Consumer，一个负责输出日志到屏幕，一个负责把日志写到磁盘上。 Exchange转发器首先我们来看看之前文章的AMQP组件流向图从图中可以看出Producer并不是直接把消息发送到queue队列中的，Producer并不知道它发送的消息要发送到哪个queue队列以及是否已经到达queue队列。消息是通过Exchange转发器进行转发到queue队列中的，Exchange与queue队列又是通过Bindings进行绑定配置的。所以Exchange需要知道如何处理消息，该把消息放置到一个queue队列还是多个queue队列，这个规则是基于Exchange的类型决定的。我们前面的文章里介绍了Exchange转发器有三种类型：direct，topic，fanout。1234#定义fanout类型的Exchange转发器channel.exchange_declare(exchange='logs', exchange_type='fanout')#发送消息channel.basic_publish(exchange='logs', routing_key='', body=message) 可以通过上面代码中的exchange_declare方法定义Exchagne，exchange=’logs’参数表示我们定义的Exchange的名字，exchange_type=’fanout’表示我们定义的fanout类型(广播模式)的转发器。basic_publish方法发送消息，我们指定了exchange为我们定义的logs转发器，并没有指定routing_key参数，因为我们要广播发送给与logs转发器绑定的所有queue队列。 Temporary queues（临时队列）之前我们使用的队列都是提前定义的有具体名字的队列，当我们使用有具体名字的队列的时候，表示我们的所有的Consumer消费者都关心该队列里的消息，并且每个消费者只能获取里面的部分消息，因为同一个消息只会发送给与该队列连接的一个消费者。但是在这里的日志系统里，我们每个新连接的Consumer消费者需要接受所有的日志消息，并且只关心当前的日志消息，在该Consumer连接前的日志却并不关系。所以我们需要在每个新的Consumer进行连接的时候申明一个新queue队列，名字我们并不关心，因为只有该consumer使用，并且该queue在该Consumer关闭的时候可以自动的删除。1234#定义新队列 exclusive=True 表示关闭连接的时候自动删除该queue对立result = channel.queue_declare(exclusive=True)#获取临时队列的名称queuename = result.method.queue 使用上面的代码可以为我们创建一个会自动删除的临时队列，队列的名字由RabbitMQ Server随机命名，比如：’amq.gen-Zg3NrcqQg5gSupdVB0AtCA’ Bindings（绑定）我们创建了fanout类型的Exchange还有临时队列，现在需要把两者关联起来，这个就需要用到Bindings了，不然Exchange怎么知道把消息发送给哪个queue呢。1channel.queue_bind(exchange='logs', queue=queuename) 完整实例Producer：1234567891011121314151617181920#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='logs', exchange_type='fanout')for num in range(1, 7): time.sleep(2) message = '[%s]-log-error-%s' % (time.strftime('%H:%M:%S', time.localtime(time.time())), str(num)) channel.basic_publish(exchange='logs', routing_key='', body=message) print(" [x] Sent %r" % (message,))connection.close() Consumer 模拟打印日志到屏幕：12345678910111213141516171819202122232425262728293031#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='logs', exchange_type='fanout')result = channel.queue_declare(exclusive=True)queuename = result.method.queue#绑定Exchange与queuechannel.queue_bind(exchange='logs', queue=queuename)print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] 模拟打印日志到屏幕 %r' % (body,)) print('[x] done') ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue=queuename)channel.start_consuming() Consumer 模拟写入日志到磁盘：12345678910111213141516171819202122232425262728293031#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.164.128', port='5672', credentials=user_pwd))channel = connection.channel()channel.exchange_declare(exchange='logs', exchange_type='fanout')result = channel.queue_declare(exclusive=True)queuename = result.method.queue#绑定Exchange与queuechannel.queue_bind(exchange='logs', queue=queuename)print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] 模拟写入日志到磁盘 %r' % (body,)) print('[x] done') ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue=queuename)channel.start_consuming() 两个Consumer代码基本一样就是模拟打印print的文字不同而已，我们需要首先启动Consumer，如果先启动Producer是没有效果的，我们先启动一个Consumer，然后启动Producer程序，Producer会发送6条日志消息，由于我们设置了每发送一条消息休眠2秒钟，所以当发送了3条的时候，我们马上开启另外一个Consumer，可以从打印的日志看到，我们后开启的Consumer会从新日志开始全部接收。打印的日志： 看看我们管理界面里出现了两个随机命名的queue队列：]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-消息确认机制与持久化存储]]></title>
    <url>%2F2018%2F04%2F29%2Frabbitmq-ackdurable%2F</url>
    <content type="text"><![CDATA[ACK 消息确认机制上一节我们介绍了RabbitMQ的分发机制有Round-robin（循环分发）与Fair dispatch（公平分发），其中的公平分发机制就必须使用到消息确认机制来保证RabbitMQ Server知道Consumer已经处理完成了任务，从而继续发送新任务给该Consumer处理，RabbitMQ Server没有使用超时机制，仅通过Consumer的连接是否中断，如果中断前还没有收到确认消息表示消息没有被成功处理。 使用消息确认机制还能保证消息不丢失，Consumer成功处理消息后发送确认消息，RabbitMQ Server才会任务消息被成功处理，然后才会把消息标记为已完成从queue中删除，如果Consumer异常或者未发送确认消息，RabbitMQ则认为消息并没有成功被处理，后续会再发送给其他Consumer进行处理，直到收到对应的确认消息，这样就能保证消息不丢失，能被正真的处理。 如果不使用确认机制，那么RabbitMQ Server 发送的消息Consumer接收以后，不管Consumer是否处理完成，直接把消息标记为完成从queue队列中删除。 我们继续使用上面的公平分发里的Consumer代码来模拟一个处理过程中关闭程序，看下消息是否还保存再队列中，该代码里我们已经开启了手动消息确认机制(默认打开) ，然后回调函数的ch.basic_ack(delivery_tag=method.delivery_tag)这句代码表示确认消息。我们使用上一节开头提供的Producer代码发送一条消息，然后开启一个Consumer可以把休眠时间设置长一点10秒把，再任务未处理完成前关闭程序，然后到管理界面查看消息是否还存在。 开启Producer发送一条消息： 管理界面 hello队列中可以看到刚才1条刚才发送的消息： 开启Consumer接受消息进行处理 可以看到消息并没有处理完成因为没有打印 [x] done ： 当RabbitMQ Server把消息发送给Consumer的后再没有接受到应答前 管理界面 hello队列里的该消息被标记为Unacked 当我再Consumer未处理完成返回确认消息前关闭程序,那么管理界面 hello队列里的该消息又会重新被标记为Ready 等其他Consumer连接到hello队列的时候，会再次发送该消息。 持久化存储消息确认机制虽然可以从消息处理流程上保证消息不丢失，但这只仅限于RabbitMQ Server正确运行的条件下，万一我们的RabbitMQ Server异常了或者我们的服务器重启了，由于我们的队列以及消息数据默认保存在内存中，所以肯定就丢失了。所以我们必须开启消息持久化能力，让数据能保存到磁盘上，防止数据的丢失。首先我们在创建队列的时候要指定为持久化，加入参数durable=True1channel.queue_declare(queue='hello', durable=True) 如果我们已经存在名称为hello的队列，那么这个语句不会起作用，可以到管理页面找到该队列，进入该队列的明细页面先删除该队列。然后发送消息的时候也要指明持久化123456channel.basic_publish(exchange='', routing_key="hello", body=message, properties=pika.BasicProperties( delivery_mode = 2, #设置消息持久化 )) 虽然我们使用消息确认机制跟持久化能保证消息不丢失，但是这个并不是百分之百的能保证所有消息不丢失，RabbitMQ Server 并不是收到收到每一条消息就同步进行持久化操作写入磁盘的，这样肯定会验证的影响性能，可能是先缓存起来了还没来得及写入磁盘，那么这个时候crash掉了，这部分还没有来得及存储的消息肯定就会丢失掉。 代码示例Producer：1234567891011121314151617181920#!/usr/bin/env pythonimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.0.110', port='5672', credentials=user_pwd))channel = connection.channel()#定义队列设置持久化durable=Truechannel.queue_declare(queue='hello', durable=True)for num in range(1, 2): message = 'hello world ' + str(num) #发送消息设置持久化delivery_mode=2 channel.basic_publish(exchange='', routing_key='hello', body=message, properties=pika.BasicProperties(delivery_mode=2)) print(" [x] Sent %r" % (message,))connection.close() Consumer：12345678910111213141516171819202122232425262728#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.0.110', port='5672', credentials=user_pwd))channel = connection.channel()channel.queue_declare(queue='hello', durable=True)channel.basic_qos(prefetch_count=1)print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(1) print('[x] done %s' % (time.strftime('%H:%M:%S', time.localtime(time.time())),)) ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue='hello')channel.start_consuming() 调用一次Producer程序然后查看下管理界面的hello队列，看到创建的hello队列有个Features为durable:true表示为持久化队列，里面存储了一条我们刚刚发送的消息 我们可以重启RabbitMQ Server看看我们的hello消息队列以及里面的那条消息是否还在。我们要持久化发送的消息的前提必须首先保证我们的消息队列是可持久化的。]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-分发机制]]></title>
    <url>%2F2018%2F04%2F28%2Frabbitmq-distribution%2F</url>
    <content type="text"><![CDATA[前言上一节的最后部分展示了一个基本的RabbitMQ发送接收的Hello World程序，Porducer发送消息，Consumer订阅队列接受消息然后打印消息，但是在实际应用场景中可能更加复杂些，如此简单的发送接受并不能满足实际需求，有可能Consumer端会有耗时计算等情况存在，所以RabbitMQ Server需要有一定的发送机制来平衡每个Consumer的负载，还有消息如何持久化，如何保证消息投递成功不丢失等 分发机制我们先模拟下Consumer进行耗时操作，看看消息是如何分发的Producer：123456789101112131415161718#!/usr/bin/env pythonimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='172.16.200.122', port='5672', credentials=user_pwd))channel = connection.channel()channel.queue_declare(queue='hello')for num in range(1, 8): message = 'hello world ' + str(num) channel.basic_publish(exchange='', routing_key='hello', body=message) print(" [x] Sent %r" % (message,))connection.close() Consumer：12345678910111213141516171819202122232425#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='172.16.200.122', port='5672', credentials=user_pwd))channel = connection.channel()channel.queue_declare(queue='hello')print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(5) print('[x] done %s' % (time.strftime('%H:%M:%S',time.localtime(time.time())),)) channel.basic_consume(callback, queue='hello', no_ack=True)channel.start_consuming() 上面的代码Consumer代码使用time.sleep(5)使线程休眠5秒来模拟耗时操作，Producer使用range(1, 8) 循环发送了7个消息，我开启了两个Consumer来接受消息，我们来看看Consumer打印的日志Consumer1Consumer2Producer Round-robin（循环分发）从打印的日志可以看出两个Consumer轮流消费了发送的7个消息，在默认的设置下，RabbitMQ会逐个发送消息到注册了该队列的消费者列表的下一个消费者，已经提前一次性分配，并不会考虑消费者处理能力的快慢，我们可以尝试把其中一个Consumer的休眠时间设置长一点，会发现两个Consumer还是在轮流的接受消息，这种处理方式叫轮询分发RabbitMQ默认使用的就是轮询分发机制，当任务加重Consumer处理不过来的时候，可以通过创建更多的Consumer来分摊任务的处理 Fair dispatch（公平分发）循环分发虽然可以通过增加更多的Consumer来增加处理消息的能力，但是循环分发并不能监控的Consumer的处理能力，它只管逐个的发送消息，这样有可能有的Consumer处理能力强，有的Consumer处理能力弱点，结果处理能力强的Consumer把消息处理完后空闲了，处理能力弱的Consumer还在艰难的处理消息。下面我把上面的例子稍微修改下，把其中一个Consumer的休眠时间修改为5秒time.sleep(5)，其他代码不变看下答应的日志情况Consumer休眠1秒：Consumer休眠5秒：可以看到休眠1秒的在28秒的时候就已经处理完了分配的4个任务，休眠5秒的第一个任务处理完已经是29秒了，最后一个39秒处理完成。可见分配并不太合理。所有RabbitMQ为我们提供了另一种分配机制公平分发通过在Consumer端设置 channel.basic_qos(prefetch_count=1), 并且在处理完任务后设置ch.basic_ack(delivery_tag=method.delivery_tag)，RabbitMQ就会在接受到该Consumer的ack应答钱不会在把新的任务给该Consumer了 Consumer代码：12345678910111213141516171819202122232425262728#!/usr/bin/env pythonimport timeimport pikauser_pwd = pika.PlainCredentials(username='admin', password='123123')connection = pika.BlockingConnection(pika.ConnectionParameters( host='172.16.200.122', port='5672', credentials=user_pwd))channel = connection.channel()channel.queue_declare(queue='hello')channel.basic_qos(prefetch_count=1)print(' [*] Waiting for messages. To exit press CTRL+C')def callback(ch, method, properties, body): print('[x] receive %r' % (body,)) time.sleep(5) print('[x] done %s' % (time.strftime('%H:%M:%S', time.localtime(time.time())),)) ch.basic_ack(delivery_tag=method.delivery_tag)channel.basic_consume(callback, queue='hello')channel.start_consuming() channel.basic_consume(callback, queue=’hello’) 默认是开启手动ack应答的，加入参数no_ack=True表示RabbitMQ Server 发送的消息Consumer接收以后，不管Consumer是否处理完成，直接把消息标记为完成从queue队列中删除。不过这种分发机制有个问题就是如果所有的Consumer都繁忙了，当需要处理的消息持续累积，队列有可能被填满，这种情况需要关注队列使用情况，合理安排Consumer的数量，或者使用别的策略如果代码中任务处理完成后忘记调用ack应答，那么会导致RabbitMQ Server不再把消息发送给该Consumer处理，那么也会导致消息队列被塞满，这个错误需要避免。 我们来看看设置公平策略后打印的日志情况，Producer还是沿用之前的代码，Consumer修改为上面提供的代码，注意修改两个Consumer休眠时间使其不一样，模拟处理能力的不同Consumer休眠1秒：Consumer休眠5秒：可以看到两个Consumer打印的日志休眠1秒处理的任务明显增多，并且从打印的时间上来看也是满足任务处理能力的。]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq 简介]]></title>
    <url>%2F2018%2F04%2F27%2Frabbitmq-introduce%2F</url>
    <content type="text"><![CDATA[AMQP背景AMQP，即Advanced Message Queuing Protocol,一个提供统一消息服务的应用层标准高级消息队列协议,是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同的开发语言等条件的限制，与2004年由摩根大通跟iMatrix开始着手该协议的开发，2006年AMQP规范发布，2007年，Rabbit公司基于AMQP标准开发的RabbtiMQ1.0发布，使用Erlang语言开发，Erlang语言由Ericson设计，专门为开发concurrent和distribution系统的一种语言。 AMQP基本概念 Broker：接受和分发消息的应用，RabbitMQ Server 就是 Message Broker Virtual Host：出于多租户和安全因素的设计，把AMQP的基本组件划分到了一个个的虚拟分组中，当多个不同的用户使用同一个RabbitMQ Server提供的服务的时候，可以划分多个Virtual host，每个用户在自己的Virtual host创建Exchange/queue等等，起到隔离作用。 Connection：Publisher/Consumer和Broker之间通过TCP连接，断开连接的操作只会在Client端，Broker不会断开，除非出现网络故障或者Broke服务异常 Channel：如果每次访问RabbitMQ都建立一个Connection连接，消息量大的时候建立TCP Connection连接开销是非常大的，效率也比较低，Channel是在Connection内部建立的逻辑连接，如果应用程序支持多线程，每个线程通常会创建单独的Channel今昔通讯，AMQP method包含了 Channel Id 帮助客户端和Message Broker识别Channel，所以不同的Channel之间是安全隔离的，Channel作为轻量级的Connection极大的减少了操作系统建立TCP Connection的开销 Exchange：message到达Broker的第一站，根据配置的分发规则，匹配查询表里的routing key，分发消息到匹配的的queue中，常用的类型有direct (point-to-point)，topic (publish-subscribe) and fanout (multicast)，topic类型的routing key支持*(匹配一个单词) #(匹配0个或者多个单词)两个通配符 Queue：消息最终配存放的地方，等待被consumer取走，一个message可以被同时拷贝到多个queue中 Binding：Exchange和queue之间的虚拟连接，binding中可以包含routing key。Binding信息被保存到exchange中的查询表中，用于message的分发依据 组件流向图 Producer Consumer Producer推送消息到Broker中，Broker内部创建好了Exchange Queue，并通过Binding将两者关联起来，Echange分发消息的时候会根据自身的类型以及Binding不同的分发策将消息放入匹配的Queue队列中，等待Consumer取走。 Exchange三种类型 Direct Direct类型的Exchange会根据发送消息时传递的routing_key去查找Binding时设置有相同的routing_key的关联队列，然后把消息存到关联的队列中，然后等待Consumer到队列中消费消息,如果没有找到关联队列，消息会被丢弃 Topic Topic类型的Exchange会根据routing key 以及通配规则，按照规则找到匹配的绑定的queue中，routing key 的值通常设置多个单词使用.连接routing key中包含两种通配符：# 可以匹配零个或者多个单词* 匹配任意单个单词 Fanout Fanout类型的Exchagne会把消息广播到所有绑定的queue中去 简单示例 Hello WorldProducer1234567891011121314151617#!/usr/bin/env pythonimport pika#连接RabbitMQ的账号密码user_pwd = pika.PlainCredentials(username='admin', password='123123')#创建connection连接connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.11.176', port='5672', credentials=user_pwd))#创建channelchannel = connection.channel()#定义queue 取名 hello，使用前最好都重新定义，如果队列已经存在会忽略该动作channel.queue_declare(queue='hello')#发送消息 使用默认的exchange 设置 routing key 为 hello，hello就是上面创建队列的名字channel.basic_publish(exchange='', routing_key='hello', body='hello world')print('[x] send hello world')connection.close() Consumer12345678910111213141516171819202122#!/usr/bin/env pythonimport pika#连接RabbitMQ的账号密码user_pwd = pika.PlainCredentials(username='admin', password='123123')#创建connection连接connection = pika.BlockingConnection(pika.ConnectionParameters( host='192.168.11.176', port='5672', credentials=user_pwd))#创建channelchannel = connection.channel()#定义queue 取名 hello，使用前最好都重新定义，如果队列已经存在会忽略该动作channel.queue_declare(queue='hello')print(' [*] Waiting for messages. To exit press CTRL+C')#定义接受消息的回调函数def callback(ch, method, properties, body): print('[x] receive %r' % (body,))#设置监听名字为hello队列channel.basic_consume(callback, queue='hello', no_ack=True)#开启监听channel.start_consuming() 参考资料 http://www.cnblogs.com/frankyou/p/5283539.html https://blog.csdn.net/anzhsoft/article/details/19563091]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ 安装]]></title>
    <url>%2F2018%2F04%2F26%2Frabbitmq-install%2F</url>
    <content type="text"><![CDATA[安装 rabbitmq-server1sudo apt-get install rabbitmq-server 添加用户 添加用户设置用户名密码 1sudo rabbitmqctl add_user username password 将用户设置为管理员（只有管理员才能远程登录） 1sudo rabbitmqctl set_user_tags username administrator 设置用户的读写等权限 1sudo rabbitmqctl set_permissions -p / username ".*" ".*" ".*" 安装RabbitMQ监控管理插件1sudo rabbitmq-plugins enable rabbitmq_management 启动插件后可以通过地址 http://localhost:15672 登陆RabbitMQ管理页面，对RabbitMQ进行管理。 基本命令启动1sudo service rabbitmq-server start 或1sudo rabbitmq-server –detached 关闭1sudo service rabbitmq-server stop 1sudo rabbitmqctl stop 插件管理开启某个插件：1rabbitmq-plugins enable xxx 关闭某个插件：1rabbitmq-plugins disable xxx 用户管理新建用户：1rabbitmqctl add_user xxx pwd 删除用户:1rabbitmqctl delete_user xxx 改密码:1rabbimqctl change_password &#123;username&#125; &#123;newpassword&#125; 设置用户角色：1rabbitmqctl set_user_tags &#123;username&#125; &#123;tag ...&#125; Tag可以为 administrator,monitoring, management 权限设置：1rabbitmqctl set_permissions [-pvhostpath] &#123;user&#125; &#123;conf&#125; &#123;write&#125; &#123;read&#125; 服务器状态：1rabbitmqctl status 队列信息：1rabbitmqctl list_queues[-p vhostpath] [queueinfoitem ...] Queueinfoitem可以为：name，durable，auto_delete，arguments，messages_ready，messages_unacknowledged，messages，consumers，memory Exchange信息：1rabbitmqctllist_exchanges[-p vhostpath] [exchangeinfoitem ...] Exchangeinfoitem有：name，type，durable，auto_delete，internal，arguments. Binding信息：1rabbitmqctllist_bindings[-p vhostpath] [bindinginfoitem ...] Bindinginfoitem有：source_name，source_kind，destination_name，destination_kind，routing_key，arguments Connection信息：1rabbitmqctllist_connections [connectioninfoitem ...] Connectioninfoitem有：recv_oct，recv_cnt，send_oct，send_cnt，send_pend等。 Channel信息：1rabbitmqctl list_channels[channelinfoitem ...] Channelinfoitem有consumer_count，messages_unacknowledged，messages_uncommitted，acks_uncommitted，messages_unconfirmed，prefetch_count，client_flow_blocked RabbitMQ管理界面]]></content>
      <categories>
        <category>RabbitMQ入门</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 单机多实例部署]]></title>
    <url>%2F2018%2F01%2F08%2Fdocker-elk-two%2F</url>
    <content type="text"><![CDATA[前言 上一篇文章docker下的elk环境搭建我们介绍了docker下的elk各个组件的部署，走通了从采集日志到查询日志的流程，但是我们之前部署的elasticsearch组件采用的单实例部署，这样并不能很好的利用系统资源，并且也不安全，如果仅有的一个实例出现了问题那么整个服务就瘫痪了，而且数据配置等都保存在docker容器中，这样也不利于我们管理维护，所以接下来我们就要解决这两个问题。 解决方案 elasticsearch 设置多个配置文件，一个实例对应一个配置文件 使用docker的数据卷功能将服务器的路径映射到docker中，使elasticsearch的数据和配置直接保存到服务器中 实施步骤docker 容器启动 我们使用数据卷功能将服务器的路径映射到我们要启动的docker容器内1$ docker run -p 5044:5044 -p 8080:80 -p 9200:9200 -p 9100:9100 -v /home/tiger/Downloads/elk:/home/tiger/elk -d elk:ubuntu /run.sh -v 参数就是启用数据卷配置，/home/tiger/Downloads/elk:/home/tiger/elk 表示把服务器的/home/tiger/Downloads/elk目录映射到docker容器的/home/tiger/elk目录上，那么你在容器内访问/home/tiger/elk里的内容实际上就是在访问服务器的/home/tiger/Downloads/elk内的内容。 elasticsearc 配置数据以及配置文件路径创建 我们在服务器上建立专门存放elasticsearch数据跟配置的文件夹,/home/tiger/Downloads/elk/elasticsearch是我自己建的路径，这个你们可以按照各自情况创建路径123$ mkdir -p /home/tiger/Downloads/elk/elasticsearch$ cd /home/tiger/Downloads/elk/elasticsearch$ mkdir -p ./elasticsearch/node1 ./elasticsearch/node2 我们只做双实例的配置所以我建立了两个节点的文件夹 node1、node2，多个实例以此类推，配置都是类似的。 然后我们进入docker容器，把elasticsearh的默认配置拷贝到这两个节点文件夹里12345$ docker start 35d535f88f45 --启动容器$ docker exec -it 35d /bin/bash -- 附加到容器root@35d535f88f45:/# su tiger --切换用户tiger@35d535f88f45:/$ tiger@35d535f88f45:/$ cd /opt/elasticsearch-6.1.1/ --进入elasticsearch目录 我们现在开始拷贝配置，把elasticsearch里的config、logs、data文件夹都拷贝到node1、node2文件夹中12tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ cp -r ./config ./data ./logs ~/elk/elasticsearch/node1/ --拷贝到node1tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ cp -r ./config ./data ./logs ~/elk/elasticsearch/node2/ --拷贝到node2 现在我们看看我们服务器刚才创建的/home/tiger/Downloads/elk/elasticsearch目录下的node1 和node2里面的内容123456789101112 $ ll node1 node2node1:总用量 12drwxr-xr-x 2 tiger tiger 4096 1月 8 00:15 configdrwxrwxr-x 3 tiger tiger 4096 1月 8 09:37 datadrwxrwxr-x 2 tiger tiger 4096 1月 8 09:37 logsnode2:总用量 12drwxr-xr-x 2 tiger tiger 4096 1月 8 09:46 configdrwxrwxr-x 3 tiger tiger 4096 1月 8 09:50 datadrwxrwxr-x 2 tiger tiger 4096 1月 8 09:50 logs config、data、logs已经复制进去了 各节点elasticsearch.yml配置 现在可以配置node1 跟node2 文件夹里config下的elasticsearch.yml文件了，config里拷贝过来了三个文件12345 $ ll ./node1/config 总用量 16-rw-rw---- 1 tiger tiger 3841 1月 8 00:15 elasticsearch.yml-rw-rw---- 1 tiger tiger 2672 1月 7 22:58 jvm.options-rw-rw---- 1 tiger tiger 5091 1月 7 22:58 log4j2.properties node1 节点的elasticsearch.yml文件配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:# es默认的集群名称是elasticsearch，注意，集群中是以name来区分节点属于那个集群的。cluster.name: tiger-elasticsearch## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#node.name: node-1## Add custom attributes to the node:##node.attr.rack: r1#是否让这个节点作为默认的master，若不是，默认会选择集群里面的第一个作为master，es有一套选择那个节点作为master的机制node.master: true# ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#配置节点数据存放的目录path.data: /home/tiger/elk/elasticsearch/node1/data## Path to log files:#配置节点日志存放的目录path.logs: /home/tiger/elk/elasticsearch/node1/logs## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 0.0.0.0## Set a custom port for HTTP:#http.port: 9200## For more information, consult the network module documentation.##es集群节点之间的通信端口号。默认9300.transport.tcp.port: 9300# --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]#集群多播时发现其他节点的主机列表， 真实多机集群环境下，这里会是多个主机的IP列表，默认格式“host：port”的数组discovery.zen.ping.unicast.hosts: ["127.0.0.1:9300"]## Prevent the "split brain" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: ## For more information, consult the zen discovery module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true#默认情况下，是不建议单机启动多个node的，这里这个参数，就是告知es单机上启动了几个实例，这里我们配置2个，若是要配置3个或者更多实例，修改这个数字即可node.max_local_storage_nodes: 2# plugs config http.cors.enabled: true http.cors.allow-origin: "*" node2 节点的elasticsearch.yml文件配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:#cluster.name: tiger-elasticsearch## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#node.name: node-2## Add custom attributes to the node:##node.attr.rack: r1#node.master: false# ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /home/tiger/elk/elasticsearch/node2/data## Path to log files:#path.logs: /home/tiger/elk/elasticsearch/node2/logs## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 0.0.0.0## Set a custom port for HTTP:#http.port: 9201## For more information, consult the network module documentation.#transport.tcp.port: 9301# --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]#discovery.zen.ping.unicast.hosts: ["0.0.0.0:9300"]## Prevent the "split brain" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: ## For more information, consult the zen discovery module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: truenode.max_local_storage_nodes: 2# plugs config http.cors.enabled: true http.cors.allow-origin: "*" elasticsearch服务启动 我网上查资料说启动elasticsearch服务的时候指定配置文件路径使用的参数是 -Des.path.conf ，我发现不行，后来网上看说是用 -Epath.conf 我测试也不行，查了好久资料都没有解决，后来看官网说可以设置ES_PATH_CONF环境变量来设置配置参数 启动 node1，打印正常日志就表示启动好了123tiger@35d535f88f45: cd /opt/elasticsearch-6.1.1tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ export ES_PATH_CONF=/home/tiger/elk/elasticsearch/node1/config --设置ES_PATH_CONF到你的配置路径文件夹tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ ./bin/elasticsearch -p /home/tiger/elk/elasticsearch/node1.pid --启动 启动 node2，打印正常日志就表示启动好了123tiger@35d535f88f45: cd /opt/elasticsearch-6.1.1tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ export ES_PATH_CONF=/home/tiger/elk/elasticsearch/node2/config --设置ES_PATH_CONF到你的配置路径文件夹tiger@35d535f88f45:/opt/elasticsearch-6.1.1$ ./bin/elasticsearch -p /home/tiger/elk/elasticsearch/node2.pid --启动 如果发现其他便捷的启动方式可以告诉下我。 结束 我们请求 http://localhost:9200/_cluster/health 可以看到返回了1234567891011121314151617&#123;cluster_name: "tiger-elasticsearch",status: "green",timed_out: false,number_of_nodes: 2,number_of_data_nodes: 2,active_primary_shards: 9,active_shards: 18,relocating_shards: 0,initializing_shards: 0,unassigned_shards: 0,delayed_unassigned_shards: 0,number_of_pending_tasks: 0,number_of_in_flight_fetch: 0,task_max_waiting_in_queue_millis: 0,active_shards_percent_as_number: 100&#125; status 状态是 green 表示健康，因为我们有了双实例，有一个充当了副本做了备份。number_of_nodes: 2 表示有两个节点 我们使用的是单机做实验，正式使用肯定是多台服务器做分布式部署，才能真正发挥威力。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>elasticsearch</tag>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker下的elk环境搭建]]></title>
    <url>%2F2018%2F01%2F04%2Fdocker-elk-create%2F</url>
    <content type="text"><![CDATA[elk结构图 先来张elk的结构图，对我们将搭建的环境有个基本的了解 用户通过Nginx反向代理请求到Kibana进行Web操作，Kibana封装了Elasticsearch，提供Web接口进行可视化操作，Logstash进行日志收集，将收集推送给Elasticsearch存储，由于Logstash比较重占用资源比较可观，所以我们使用轻量级的Filebeat在客户服务器上进行日志收集然后推送给Logstash，Logstash再推送给Elasticsearch存储，如果日志量比较大，可能会对FIlebeat到Logstash这个管道处理产生比较大负荷，所以我们可以在这部分的中间插入一个队列进行缓冲，比如redis或者kafka。 各个组件安装 接下来我们就在docker里安装各个用到的组件，由于我自己的电脑使用的ubuntu系统比较熟悉，所以我们还是使用ubuntu的环境，由于有几个组件是基于java开发的，所以运行需要java环境，需要jdk8以上，如何安装java环境这里就不操作了。 docker环境准备 我们首选需要一个docker的ubuntu的基础官方镜像 1$ docker search ubuntu 这个命令可以查询ubuntu相关的镜像文件，是按照星级排序的，OFFICIAL 一栏的[OK]表示是官方镜像，我们使用下面的命令进行镜像下载。 1$ docker pull ubuntu 启动ubuntu镜像12$ docker run -t -i ubuntu:latest /bin/bashroot@51a064df9e85:/# 我们新建一个用户，后面会用到1234567891011121314151617root@51a064df9e85:/# adduser tigerAdding user `tiger' ...Adding new group `tiger' (1000) ...Adding new user `tiger' (1000) with group `tiger' ...Creating home directory `/home/tiger' ...Copying files from `/etc/skel' ...Enter new UNIX password: Retype new UNIX password: passwd: password updated successfullyChanging the user information for tigerEnter the new value, or press ENTER for the default Full Name []: Room Number []: Work Phone []: Home Phone []: Other []: Is the information correct? [Y/n] y 我们已经运行镜像创建了ubuntu的容器，我们现在就在容器里安装各个组件。首先申明下，这个ubuntu是简化版可能很多命令没有包括在这个镜像内，需要自行安装，如何安装可以百度。 Elasticsearch安装 下载地址：https://www.elastic.co/downloads/elasticsearch， 现在最新的是6.1.1，图有点老了。 切换到刚才创建的用户，并且建立一个目录用来存放下载的文件，然后进入该目录进行下载1234root@51a064df9e85:/# su tigertiger@51a064df9e85:/$ mkdir ~/downloadstiger@51a064df9e85:/$ cd ~/downloads/tiger@51a064df9e85:~/downloads$ 1tiger@51a064df9e85:~/downloads$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.1.1.tar.gz 解压文件然后吧解压出来的文件夹移动到/opt/下，我一般喜欢把安装文件放到/opt/目录下1234tiger@51a064df9e85:~/downloads$ tar -zxvf elasticsearch-6.1.1.tar.gztiger@51a064df9e85:~/downloads$ sudo mv elasticsearch-6.1.1 /opt/tiger@51a064df9e85:~/downloads$ cd /opt/elasticsearch-6.1.1/tiger@46a32b71c85e:/opt/elasticsearch-6.1.1$ 进入到/opt/elasticsearch-6.1.1/目录进行配置文件修改1tiger@46a32b71c85e:/opt/elasticsearch-6.1.1$ vim ./config/elasticsearch.yml 找到network.host: 配置 取消注释 修改值为 0.0.0.0 然后就可以启动elasticsearch服务了，我这个是手动启动服务，要开机运行可以自己百度一下。1tiger@46a32b71c85e:/opt/elasticsearch-6.1.1$ ./bin/elasticsearch 打印一些日志后服务就启动好了，可以使用 curl ‘http://localhost:9200‘ 测试下服务是否正常。12345678910111213141516tiger@46a32b71c85e:/$ curl 'http://localhost:9200'&#123; "name" : "KdeFZ6P", "cluster_name" : "elasticsearch", "cluster_uuid" : "H4vGKoPnRPOxtywv-G6dkQ", "version" : &#123; "number" : "6.1.1", "build_hash" : "bd92e7f", "build_date" : "2017-12-17T20:23:25.338Z", "build_snapshot" : false, "lucene_version" : "7.1.0", "minimum_wire_compatibility_version" : "5.6.0", "minimum_index_compatibility_version" : "5.0.0" &#125;, "tagline" : "You Know, for Search"&#125; Kibana安装 下载地址：https://www.elastic.co/downloads/kibana 现在最新的是6.1.1，图有点老了。 按照Elasticsearch的下载安装步骤将解压缩出来的文件夹移动到/opt/目录下，然后进行Kibana的配置工作，主要是 server.host 以及 elasticsearch.url 的配置。 server.host 的值 如果我们使用nginx做代理那么 值可以是 localhost，如果需要在容器外部访问 设置为 0.0.0.0 elasticsearch.url的值设置为 elasticsearch的服务地址 由于我们是在同一个容器内默认的就可以 elasticsearch.url: http://localhost:9200进入kibana-6.1.1-linux-x86_64目录启动Kibana 1tiger@46a32b71c85e:/opt/kibana-6.1.1-linux-x86_64$ ./bin/kibana Kibana的访问地址是 http://localhost:5601 Logstash安装 下载地址：https://www.elastic.co/downloads/logstash 最新是6.1.1版本，按照Elasticsearch的下载安装步骤将解压缩出来的文件夹移动到/opt/目录下，然后进行Logstash的配置工作我们在/opt/logstash-6.1.1/config/目录下新建一个配置logstash-simple.conf，进行如下配置12345678910111213141516171819input &#123; beats &#123; port =&gt; "5044" &#125;&#125; filter &#123; grok &#123; match =&gt; &#123; "message" =&gt; "%&#123;COMBINEDAPACHELOG&#125;"&#125; &#125; geoip &#123; source =&gt; "clientip" &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [ "localhost:9200" ] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 由于我们是用Filebeat在客户服务器上启动收集日志的，所以我们开启5044端口监听，接受日志输入，具体配置文件的含义可以自行百度了解。 进入logstash-6.1.1目录启动Logstash1tiger@46a32b71c85e:/opt/logstash-6.1.1$ ./bin/logstash -f ./config/logstash-simple.conf Filebeat安装 Filebeat工具是安装到需要进行日志收集的机器上的，由于我的机器是ubuntu所以我下载ubuntu对应的版本，下载地址：https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.1.1-linux-x86_64.tar.gz 最新是6.1.1版本，解压到某个目录下，我的是放在/opt/filebeat-6.1.1-linux-x86_64/目录下的，然后进行filebeat的配置工作 打开配置文件/opt/filebeat-6.1.1-linux-x86_64/filebeat.yml，简单的配置如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849filebeat.prospectors:- type: log # Change to true to enable this prospector configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - c:\programdata\elasticsearch\logs\* # Filebeat以多快的频率去prospector指定的目录下面检测文件更新（比如是否有新增文件） # 如果设置为0s，则Filebeat会尽可能快地感知更新（占用的CPU会变高）。默认是10s scan_frequency: 100s # 设定Elasticsearch输出时的document的type字段 可以用来给日志进行分类。Default: log document_type: log ### Multiline options # Mutiline can be used for log messages spanning multiple lines. This is common # for Java Stack Traces or C-Line Continuation # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ multiline.pattern: ^\[ # Defines if the pattern set under pattern should be negated or not. Default is false. multiline.negate: true # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern # that was (not) matched before or after or as long as a pattern is not matched based on negate. # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash multiline.match: after # 合并的最多行数（包含匹配pattern的那一行） #max_lines: 500 # 如果设置为true，Filebeat从文件尾开始监控文件新增内容，把新增的每一行文件作为一个事件依次发送， # 而不是从文件开始处重新发送所有内容 tail_files: true #============================= Filebeat modules =============================== filebeat.config.modules: # Glob pattern for configuration loading #path: $&#123;path.config&#125;/modules.d/*.yml path: /opt/filebeat-6.1.1-linux-x86_64/filebeat.yml # Set to true to enable config reloading reload.enabled: true # Period on which files under path should be checked for changes #reload.period: 10s #----------------------------- Logstash output -------------------------------- output.logstash: # The Logstash hosts hosts: ["localhost:5044"] index: "hubei-solution-%&#123;+yyyy.MM.dd&#125;" # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"] # Certificate for SSL client authentication #ssl.certificate: "/etc/pki/client/cert.pem" # Client Certificate Key #ssl.key: "/etc/pki/client/cert.key" filebeat.prospectors.paths里的地址修改为你日志的地址，可以采用通配符*进行模糊匹配 multiline部分是对多行日志的设置，比如错误日志打印堆栈信息的就会有多行的情况，这个配置如何识别多行为一行 filebeat.config.modules.path设置全局配置文件的地址，可以设置你自己配置文件存放路径 我们输出到Logstash，所以我们开启Logstash output栏的配置，把其他输出设置注释掉，配置好Logstash的输入监听地址 进入Filebeat目录开启服务 -e参数会打印日志到前台1$ ./filebeat -e Nginx安装 我们使用ubuntu的 apt-get进行nginx的安装1$ sudo apt-get install nginx 使用openssl创建一个管理员（admin），这是用来登录Kibana web接口的：12$ sudo -v$ echo "admin:`openssl passwd -apr1`" | sudo tee -a /etc/nginx/htpasswd.users 按照提示设置admin用户的密码。编辑Nginx配置文件：1$ sudo vim /etc/nginx/sites-available/default 替换为下面的配置：1234567891011121314151617server &#123; listen 80; server_name your_domain_or_IP; auth_basic "Restricted Access"; auth_basic_user_file /etc/nginx/htpasswd.users; location / &#123; proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; &#125;&#125; 把your_domain_or_IP替换为你服务器的IP或域名；检查Nginx配置语法：1$ sudo nginx -t 指定配置文件进行启动Nginx：1$ nginx -c /etc/nginx/sites-available/default 开启相关端口映射 我们已经安装好了各个需要用到的组件，但是我们像上面启动各个服务只能在容器内部访问到，外面是不能访问的，所以我们需要开放需要让外部访问的端口，比如Logstash监听的5044端口，nginx配置监听的8080端口，或者如果希望外面能访问elasticsearch的restfull api 也可以开放配置的9200端口 docker 端口映射只需要在run 命令 带上 -p 9200:9200 就可以了 -p 表示映射指定的端口 -P 表示随机指定一个端口映射到docker容器 我们先关闭之前开启的容器，后面的一串字符串是你容器的ID1$ docker stop 9929c83b1f74 最好吧我们刚才修改的容器保存为镜像以免丢失，elk:ubuntu 是我取的镜像的名称1$ docker commit 9929c83b1f74 elk:ubuntu 重新运行镜像创建新容器，我们映射了三个端口1$ docker run -p 5044:5044 -p 8080:80 -p 9200:9200 -d elk:ubuntu /run.sh 现在我们访问下 http://localhost:8080 ，输入前面安装Nginx设置的用户名密码就可以看到Kibana的界面了 总结 目前我们只是简单的搭建了一个elk的环境，跑通了从采集日志到查询展示日志也就是从Filebeat到Kibana的各个环节，但是各个组件的配置还有很多需要了解学习，elasticsearch还有很多知识需要学习，以后也会把自己学习实践到的东西继续写出来 参考文章：https://es.xiaoleilu.com/index.htmlhttp://blog.topspeedsnail.com/archives/4825http://www.linuxidc.com/Linux/2017-09/147092p2.htmhttps://www.cnblogs.com/sunxucool/p/3799190.htmlhttp://blog.csdn.net/chengxuyuanyonghu/article/details/54378778http://m.blog.csdn.net/u012516166/article/details/74946823]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>elasticsearch</tag>
        <tag>elk</tag>
        <tag>kibana</tag>
        <tag>logstash</tag>
        <tag>filebeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取github中创建静态站点的用户信息]]></title>
    <url>%2F2017%2F12%2F17%2FGithubPageUsersAcquire%2F</url>
    <content type="text"><![CDATA[功能介绍 最近在[GitHub Pages]:(https://pages.github.com/) 上使用hexo创建了一个静态博客，觉得GitHub提供的这个服务蛮有意思的，所以突发奇想想看看到底有多少用户使用GItHub Pages创建了自己的静态站点，所以就写了个python的小爬虫想爬一爬GitHub的用户数据。 经过搜索了解到GitHub提供非常好用的[API]:(https://developer.github.com/v3/) ，可以非常方便的获取用户各种信息。 使用工具 操作系统 ubuntu16.04 语言python3 爬虫框架pyspider，该框架是国人开发的强大的开源爬虫框架 代理工具squid，由于github对同一个ip调用api的次数有限制，超过次数会被限制访问，所以需要代理访问。爬取思路获取用户个人信息REST API https://api.github.com/users/{username}通过传入用户登录名就可以获取到该用户的个人信息，例如调用https://api.github.com/users/tigerphz 获取到返回的json数据，下面仅列出重点数据 12345678910111213141516&#123;login: "tigerphz",//用户名id: 5802152,avatar_url: "https://avatars3.githubusercontent.com/u/5802152?v=4",url: "https://api.github.com/users/tigerphz",followers_url: "https://api.github.com/users/tigerphz/followers",//粉丝列表地址following_url: "https://api.github.com/users/tigerphz/following&#123;/other_user&#125;",//关注列表地址repos_url: "https://api.github.com/users/tigerphz/repos",//项目列表地址type: "User",site_admin: false,name: "tigerphz",public_repos: 20,//公共项目个数public_gists: 0,followers: 1,//粉丝人数following: 3//关注人数&#125; 获取项目信息REST API https://api.github.com/users/{username}/repos?page={page}通过该接口可以拿到用户的项目列表，通过上面获取到的public_repos: 20 表示该用户有20个公共项目，可以计算出需要请求的总页数，默认每页显示30个项目。下面紧列出重点数据123456789101112131415161718192021[&#123;id: 70320275,name: "aspnetboilerplate",full_name: "tigerphz/aspnetboilerplate",owner: &#123;login: "tigerphz",id: 5802152,avatar_url: "https://avatars3.githubusercontent.com/u/5802152?v=4",gravatar_id: "",url: "https://api.github.com/users/tigerphz",html_url: "https://github.com/tigerphz",followers_url: "https://api.github.com/users/tigerphz/followers",following_url: "https://api.github.com/users/tigerphz/following&#123;/other_user&#125;",type: "User",site_admin: false&#125;,private: false,html_url: "https://github.com/tigerphz/aspnetboilerplate"&#125;] name: “aspnetboilerplate” 就是项目名称，由于GItHub Pages的规定项目名称必须是username.github.io或者username.github.com格式，所以很容易的可以初步筛选出符合规定的项目 获取关注人列表REST API https://api.github.com/users/{username}/following?page={page}通过该接口可以拿到用户的关注人列表，通过上面获取到的following: 3 表示该用户有3关注人，可以计算出需要请求的总页数，默认每页显示30个项目。下面紧列出重点数据1234567891011121314151617[&#123;login: "JakeWharton",id: 66577,followers_url: "https://api.github.com/users/JakeWharton/followers",following_url: "https://api.github.com/users/JakeWharton/following&#123;/other_user&#125;",organizations_url: "https://api.github.com/users/JakeWharton/orgs",repos_url: "https://api.github.com/users/JakeWharton/repos"&#125;,&#123;login: "stone0090",id: 1546345,followers_url: "https://api.github.com/users/stone0090/followers",following_url: "https://api.github.com/users/stone0090/following&#123;/other_user&#125;",repos_url: "https://api.github.com/users/stone0090/repos"&#125;] 通过上面的信息我们可以拿到我们关注人的个人信息，进而可以获取到关注人的项目信息、关注人列表、粉丝列表 获取粉丝列表REST API https://api.github.com/users/{username}/followers?page={page}通过该接口可以拿到用户的关注人列表，通过上面获取到的followers: 1 表示该用户有1个粉丝，可以计算出需要请求的总页数，默认每页显示30个项目。123456789[&#123;login: "LowApe",id: 25314586,followers_url: "https://api.github.com/users/LowApe/followers",following_url: "https://api.github.com/users/LowApe/following&#123;/other_user&#125;"repos_url: "https://api.github.com/users/LowApe/repos"&#125;] 通过上面的信息我们可以拿到我们粉丝的个人信息，进而可以获取到粉丝的项目信息、关注人信息、粉丝列表 只需要这四个接口，我们就可以遍历整个用户关系网获取到非常多用户数据。 关键代码pyspider入口下面的代码是pyspider任务执行入口，会调用on_start方法开始执行任务，代码里有一些关键配置，比如代理ip设置，获取随机的通用请求头信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# -*- coding=utf8 -*-""" pyspider任务"""import loggingfrom pyspider.libs.base_handler import *from pyspider.libs.header_switch import HeadersSelectorfrom ..settings import BASE_USER_URL_LISTfrom . import user_relationship_analysis as analysisLOGGER = logging.getLogger(__name__)class Handler(BaseHandler): header_helper = HeadersSelector() header_helper.set_host(host='api.github.com') crawl_config = &#123; 'headers': header_helper.get_random_header(), 'timeout': 1000, 'proxy': '127.0.0.1:3128' &#125; def __init__(self): self.base_url_list = BASE_USER_URL_LIST @every(minutes=24 * 60) def on_start(self): for url in self.base_url_list: self.crawl(url, callback=self.analysis_user) @config(age=10 * 24 * 60 * 60) def analysis_user(self, response): user, follower_urls, following_urls, repo_urls = analysis.analysis_user( response) for repo_url in repo_urls: self.crawl(repo_url, callback=self.analysis_repo) for follower_url in follower_urls: self.crawl(follower_url, callback=self.analysis_follower) for following_url in following_urls: self.crawl(following_url, callback=self.analysis_following) @config(priority=2) def analysis_repo(self, response): analysis.analysis_repo(response) @config(age=10 * 24 * 60 * 60) def analysis_follower(self, response): user_urls = analysis.analysis_follower(response) for user_url in user_urls: self.crawl(user_url, callback=self.analysis_user) @config(age=10 * 24 * 60 * 60) def analysis_following(self, response): user_urls = analysis.analysis_following(response) for user_url in user_urls: self.crawl(user_url, callback=self.analysis_user) 用户关系解析下面的代码是进行上面介绍的四个接口地址生成使用的包括分页关键数据保存使用的redis123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# -*- coding=utf8 -*-""" 用户关系解析"""import loggingfrom ..extensions import redis_clientfrom .. import utilsLOGGER = logging.getLogger(__name__)def analysis_user(response): """解析用户 https://api.github.com/users/&#123;username&#125; :param response: :return: """ data = response.json user_id = data.get('login') if not user_id: return user = &#123;'id': user_id, 'type': data.get('type'), 'name': data.get('name'), 'company': data.get('company'), 'blog': data.get('blog'), 'location': data.get('location'), 'email': data.get('email'), 'repos_count': data.get('public_repos', 0), 'gists_count': data.get('public_gists', 0), 'followers': data.get('followers', 0), 'following': data.get('following', 0), 'created_at': data.get('created_at'), 'avatar_url': data.get('avatar_url')&#125; follower_urls = utils.get_url_list( user['id'], utils.get_user_follower_url, user['followers']) following_urls = utils.get_url_list( user['id'], utils.get_user_following_url, user['following']) repo_urls = utils.get_url_list( user['id'], utils.get_user_repo_url, user['repos_count']) redis_client.sadd('github_users', user_id) return user, follower_urls, following_urls, repo_urlsdef analysis_repo(response): """ 解析用户的项目 """ data = response.json for item in data: full_name = item['full_name'] user_name = item['owner']['login'] if not utils.check_repo_github_papers(user_name, full_name): continue redis_client.hset('github_papers', user_name, full_name)def analysis_follower(response): """ 解析用户的粉丝列表 """ data = response.json urls = [] for item in data: user_name = item['login'] urls.append(utils.get_user_page_url(user_name)) return urlsdef analysis_following(response): """ 解析用户关注列表 """ data = response.json urls = [] for item in data: user_name = item['login'] urls.append(utils.get_user_page_url(user_name)) return urls 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# -*- coding=utf8 -*-""" url地址"""from . import contsdef get_user_page_url(user_name): """ 获取用户主页url:https://api.github.com/users/&#123;user_name&#125;&#125; """ return 'https://&#123;&#125;/users/&#123;&#125;'.format(conts.GITHUB_API_HOST, user_name)def get_user_follower_url(user_name, page=1): """ 获取用户的粉丝列表rul：https://api.github.com/users/&#123;user_name&#125;/followers?page=&#123;page&#125; """ return 'https://&#123;&#125;/users/&#123;&#125;/followers?page=&#123;&#125;'.format(conts.GITHUB_API_HOST, user_name, page)def get_user_following_url(user_name, page=1): """ 获取用户关注的用户列表url：https://api.github.com/users/&#123;user_name&#125;/following?page=&#123;page&#125; """ return 'https://&#123;&#125;/users/&#123;&#125;/following?page=&#123;&#125;'.format(conts.GITHUB_API_HOST, user_name, page)def get_user_repo_url(user_name, page=1): """ 获取用户项目列表url:https://api.github.com/users/&#123;user_name&#125;/repos?page=&#123;page&#125; """ return 'https://&#123;&#125;/users/&#123;&#125;/repos?page=&#123;&#125;'.format(conts.GITHUB_API_HOST, user_name, page)def get_url_list(user_name, func, count): """ 根据总数调用func生成url """ result = [] page = 1 while (page - 1) * conts.PAGE_SIZE &lt; count: result.append(func(user_name, page)) page += 1 return resultdef check_repo_github_papers(user_name, repo_name): """ 判断项目是不是静态博客，&#123;user_name&#125;.github.com &#123;user_name&#125;.github.io Args: user_name(string) 用户名 repo_name(string) 项目全名 """ part = repo_name[len(user_name):] return '.github.com' in part or '.github.io' in part 获取到的部分用户信息1234567891011121314151617181920212223127.0.0.1:6379&gt; HGETALL github_papers 1) "tigerphz" 2) "tigerphz/tigerphz.github.com" 3) "LowApe" 4) "LowApe/didiaoyuan.github.io" 5) "stone0090" 6) "stone0090/stone0090.github.io" 7) "lfq7413" 8) "lfq7413/lfq7413.github.io" 9) "oguzcelik"10) "oguzcelik/oguzcelik.github.io"11) "astaxie"12) "astaxie/astaxie.github.io"13) "Trinea"14) "Trinea/trinea.github.com"15) "Slock"16) "Slock/Slock.github.io"17) "taoyuanxiaoqi"18) "taoyuanxiaoqi/taoyuanxiaoqi.github.io"19) "viti"20) "viti/viti.github.io"21) "kevinzh64"22) "kevinzh64/kevinzh64.github.io"]]></content>
      <categories>
        <category>python爬虫</category>
      </categories>
      <tags>
        <tag>github pages</tag>
        <tag>pyspider</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 安装]]></title>
    <url>%2F2017%2F12%2F11%2Fdocker%2F</url>
    <content type="text"><![CDATA[Docker 版本简介 Docker是基于Go语言实现的开源容器项目，诞生于2013年初，最初发起者是dotCloud公司。Docker自开源后收到广泛的关注和讨论，逐渐形成了围绕Docker的生态圈，dotCloud公司后来改名Dockker Inc，专注于Docker相关的技术和产品。Docker的主要目标是’Build,Ship and Run Any App,Anywhere’，即通过对应用组件的封装、分发、不熟、运行等生命周期的管理，达到应用组件级别的“一次封装，到处运行”，Docker基于linux的多项开源技术提供高效、敏捷和轻量级的容器方案。 目前Docker分为了两个版本Community Edition (CE) 和 Enterprise Edition (EE)社区版（CE）是免费的，面向开发者或者小团队使用的，用户可以选择stable(更新发布较慢) 和 edge(每个月都会发布新特性) 两个版本。 企业版(EE)是付费使用的 社区版（CE） Docker CE是免费的Docker产品的新名字，是为开发人员或者小团队创建基于容器的应用，Docker CE有两个更新版本 Stable 每季度发布，适用于希望追求稳定的用户，提供4个月支持 Edge 每个月发布，主要面向喜欢尝试新功能的用户，提供一个月支持 企业版(EE)Docker EE是专门为企业的发展和IT团队创建的，Docker EE为企业提供最安全的容器平台，Docker EE提供三个服务层次： 服务层级 功能 Base 包含用于认证基础设施的Docker平台Docker公司的支持经过 认证的、来自Docker Store的容器与插件 Standard 添加高级镜像与容器管理LDAP/AD用户集成基于角色的访问控制(Docker Datacenter) Advanced 添加Docker安全扫描连续漏洞监控 每个Docker EE版本都享受为期一年的支持跟维护期，在此期间接受安全与关键问题修正。 Docker CE 版本安装本次基于ubuntu系统安装，安装Docker CE, 你需要下面列出的64位的Ubuntu版本的一个 Artful 17.10 (Docker CE 17.11 Edge only) Zesty 17.04 Xenial 16.04 (LTS) Trusty 14.04 (LTS)Docker CE 在Ubutnu中支持基于x86_64, armhf, 和 s390x (IBM z Systems)的架构检查kernel版本和操作系统架构12345678910$ uname -aLinux tigerphz-pc 4.10.0-42-generic #46~16.04.1-Ubuntu SMP Mon Dec 4 15:57:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux$ cat /proc/version Linux version 4.10.0-42-generic (buildd@lgw01-amd64-007) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) ) #46~16.04.1-Ubuntu SMP Mon Dec 4 15:57:59 UTC 2017$ lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 16.04.3 LTSRelease: 16.04Codename: xenial 可以看到我们使用的是代号xenial的ubuntu 16.04 64位版本和kernel4.10内核 安装Docker CE更新apt源1$ sudo apt-get update 安装下面的包使得支持https12345$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common 添加Docker官方GPG秘钥1$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 添加系统版本对应的源 amd64 1234$ sudo add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable" armhf 1234$ sudo add-apt-repository \ "deb [arch=armhf] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable" s390x 1234$ sudo add-apt-repository \ "deb [arch=s390x] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable" 更新apt源1$ sudo apt-get update 安装 docker-ce1$ sudo apt-get install docker-ce 等待安装，安装好以后可以查看版本1234567891011121314151617$ docker version Client: Version: 17.09.1-ce API version: 1.32 Go version: go1.8.3 Git commit: 19e2cf6 Built: Thu Dec 7 22:24:23 2017 OS/Arch: linux/amd64Server: Version: 17.09.1-ce API version: 1.32 (minimum version 1.12) Go version: go1.8.3 Git commit: 19e2cf6 Built: Thu Dec 7 22:23:00 2017 OS/Arch: linux/amd64 Experimental: false 还可以输入一下命令，测试运行下是否成功安装好1$ sudo docker run hello-world 该命令会去 Docker默认的远程仓库下载名为hello-world的镜像文件到本地，然后运行输出一段信息。1234567891011121314151617181920212223$ sudo docker run hello-world [sudo] tiger 的密码： Hello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the "hello-world" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://cloud.docker.com/For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 每次运行docker需要带上sudo，比较麻烦，可以创建一个docker组然后把当前用户加入改组 12$ sudo groupadd docker$ sudo usermod -aG docker tiger 查看本地下载的镜像文件，发现了hello-world123$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest f2a91732366c 2 weeks ago 1.85kB]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F404.html</url>
    <content type="text"><![CDATA[&lt;!DOCTYPE HTML&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[about]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[categories]]></title>
    <url>%2Fcategories%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[tags]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
